{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DSCI6003-7.1 Lecture\n",
    "\n",
    "\n",
    "### By the End of This Lecture You Will Be:\n",
    "1. Familiar with what you need to know for Skills Test 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC/Profit Curves\n",
    "\n",
    "* Determining optimal ROC threshold: [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week2/2.4/2.4-LectureLabCombo.ipynb)\n",
    "* Cost Benefit Matrix and Profit Curves: [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week2/2.4/DSCI6003-2.4-Lecture.pdf)\n",
    "\n",
    "## Deeper into Trees\n",
    "\n",
    "Review the [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week4/4.3/DSCI6003-4.3.Lecture.ipynb)\n",
    "\n",
    "Trees can be used for both regression and classification. In regression, the tree returns the average of the points assigned to that leaf.\n",
    "\n",
    "\n",
    "prepruning\n",
    "\n",
    "    Construct root node:\n",
    "        while X is larger than one row or not preprune:\n",
    "            find set S = {S1, S2} that minimizes impurities\n",
    "            choose the S that minimizes the size\n",
    "            construct child nodes and pass them S\n",
    "            check preprune \n",
    "\n",
    "        check preprune:\n",
    "            if tree has reached max depth or ratio of splits is imbalanced or information gain < error:\n",
    "                 preprune = True\n",
    "\n",
    "pruning:\n",
    "\n",
    "    While not a root node or not a singleton leaf:\n",
    "        use the current tree to predict y\n",
    "        tentatively merge classes (training y) of this split\n",
    "        if the accuracy of the merge ( sum(merged_y==y_train)/float(y.shape[0])) is greater than the unmerged\n",
    "            set the current node to be a leaf\n",
    "            set the current classes to be the merged classes\n",
    "            set the current node name to be the merged name\n",
    "            delete left and right leaves.\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Review the [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week5/5.2/DSCI6003-5.2.Lecture.ipynb)\n",
    "\n",
    "Boosting is by definition the use of one model's output to train another model. It is standard to use the word \"boosting\" to specifically refer to using this process with trees. It is also important to realize that the two boosting algorithms that we present are quite different from each other in both implementation and execution.\n",
    "\n",
    "### Adaboost\n",
    "\n",
    "Works like a forest, except each member of the forest specializes on the misclassified points of the last tree. An exponential weighting scheme is applied to each misclassified point, increasing the weight in importance for successive trees. Weights are used in the splitting portion of the fitting process, creating incentive for the tree (because of the importance of the weighted points to the entropy calculation) to isolate the weighted points from each other at each new split.\n",
    "\n",
    "    Fit:\n",
    "    \n",
    "    Initialize weights W to be 1/N\n",
    "    for m in range(1,M): (M determined by user)\n",
    "        fit a standard decision tree G(X,w)\n",
    "        compute total error err <- sum(W*misclassified points)/sum(W)\n",
    "        compute alpha_i <- log((1-err)/err)\n",
    "        change weights by an exponent if misclassified w_i <- \n",
    "        w_i * exp(alpha_i)\n",
    "   \n",
    "    Predict:\n",
    "    \n",
    "    for m in range(1, M):\n",
    "        sum += alpha_m * G_m(X)\n",
    "    return sign(sum)\n",
    "\n",
    "### Gradient Boost\n",
    "\n",
    "Trains the same tree on the errors of the previous iteration. What is reported is simply an additive model. Unlike Adaboost, we do not keep a list of every model; instead there is a single multiplier, $\\lambda$, that decreases iteratively, taking the decisions of the previous tree with it.\n",
    "\n",
    "In effect we compute\n",
    "\n",
    "\n",
    "\n",
    "    Fit: \n",
    "        Build an initial tree h(X, \\theta_0) and initialize gradient\n",
    "        for t in range(1, M):\n",
    "            Compute the current gradient using a chosen loss function \\Psi\n",
    "            Fit a new tree h(X, \\theta_t)\n",
    "            Find the best gradient step-size \\lambda_t\n",
    "            \n",
    "            \\lambda_t = argmin_lambda(Psi[y_i, f_{t-1}(X)+\\lambda_{t-1}h(X, \\theta_t)])\n",
    "            Update the estimate f_t <- f_{t-1}+\\lambda_t h(X, \\theta_t)\n",
    "\n",
    "\n",
    "    Predict:\n",
    "        As a standard forest, f_t <- f_{t-1}+\\lambda_t \\sum h(X, \\theta_t)\n",
    "\n",
    "\n",
    "    Use cases: Anywhere you'd use a random forest, a good general classifier, i.e. large number of features, noisy data that are known to have logical relationships amongst each other.\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "There are a few tools that we can use in regards to model selection, the best of which are the informational criteria tools, which give us a simple measure of relative model quality based on a given model's loss function. These have been covered only very lightly. It's important that you are at least familiar with that discussed in the [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week5/5.3/DSCI6003-5.3.Lecture.ipynb)\n",
    "\n",
    "\n",
    "## K-Means/PAM\n",
    "\n",
    "Make sure to review the [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week6/6.1/DSCI6003-6.1.Lecture.ipynb) that cover an introduction to clustering as well as K-means. You are assigned the practicum this week, that expands on the clustering metrics as well as K-Means itself. Particularly, you have learned two types of clustering: Aggregative/Hierarchical and KMeans.\n",
    "\n",
    "* K-medoids Update steps (and how they are different from Kmeans): [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week6/6.2/DSCI6003-6.2-Lecture.ipynb)\n",
    "\n",
    "\n",
    "Initialize: randomly select k of the n data points as the medoids\n",
    "Associate each data point to the closest medoid. (\"closest\" here is defined using any valid distance metric, most commonly Euclidean distance, Manhattan distance or Minkowski distance)\n",
    "\n",
    "### K-Means\n",
    "\n",
    "    Asssign a characteristic number from 1 to K to each of N data points randomly\n",
    "    While cluster assignments keep changing:\n",
    "        For each of K clusters:\n",
    "            Calculate cluster centroid\n",
    "            For each of N points:\n",
    "                determine point distance to centroid\n",
    "        For each of N points:\n",
    "            Assign point to centroid it is closest to\n",
    "\n",
    "\n",
    "### PAM\n",
    "\n",
    "    While cluster assignments keep changing:\n",
    "        For each of K medoids:\n",
    "            For each non-medoid data point o:\n",
    "                Swap m and o and compute the total cost of the configuration\n",
    "\n",
    "                Select the configuration with the lowest cost\n",
    "    \n",
    "\n",
    "## Clustering Evaluation\n",
    "\n",
    "Evaluation of clustering is not really specific to the type of clustering you're using. We provided three metrics so as to essentially provide a small history of how clusters are evaluated.\n",
    "\n",
    "* Gap Statistic vs. Silhouette vs. Elbow method: [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week6/6.2/DSCI6003-6.2-Lecture.ipynb)\n",
    "\n",
    "## Expectation Maximization (and GMM)\n",
    "\n",
    "The important idea here is the basic principal behind the EM maximization algorithm. Be very familiar with the discussion in the [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week6/6.3/DSCI6003-6.3.Lecture.ipynb) on this matter. \n",
    "\n",
    "###EM: General Algorithm \n",
    "\n",
    "    While not converged:\n",
    "    Compute the expectation value of the log-likelihood function as a function of a dummy parameter. (This comes from a previously established distribution function)\n",
    "    Estimate a new parameter set by maximizing the theta parameters (this is done in GMM by recomputing the weights)\n",
    "\n",
    "\n",
    "## Proximity Clustering\n",
    "\n",
    "\n",
    "* GMM vs. DBSCAN: [notes](https://github.com/zipfian/DSCI6003-student/blob/master/week6/6.4/DSCI6003-6.4.Lecture.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
