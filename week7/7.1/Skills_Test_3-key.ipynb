{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 Skills Test 3\n",
    "\n",
    "This skills test is designed to provide a review and cover all remaining material left in the mastery skills tracker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Section (40%):\n",
    "    \n",
    "Short answers are best! \"description\" of an algorithm should be done in pseudocode. Use the pseudocode in the notes as a template.\n",
    "    \n",
    "1. Describe the use cases, weaknesses and strengths of OLS - including Multilinear and Polynomial\n",
    "    * Use cases: Regression setting where response is modeled by linear combination of features\n",
    "\n",
    "    * Strengths: Well understood, easy to train, parametric, interpretable\n",
    "\n",
    "    * Weakness: Can underfit for variety of problems, very strict assumptions for use\n",
    "\n",
    "2. Write the 6 OLS assumptions for BLUE\n",
    "    * Errors have 0 mean\n",
    "    * Features are noncolinear\n",
    "    * Errors are drawn from distribution of same finite variance (homoscedasticity)\n",
    "    * Errors are uncorrolated\n",
    "    * Errors are normally distributed around 0 with same finite variance\n",
    "    * Errors are independent\n",
    "\n",
    "3. Implement OLS in Matrix Form\n",
    "    * theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "4. Derive and write the cost function of OLS\n",
    "    * Minimize Mean squared error, which can be derived from probabilistic interpretation of OLS (cf. CS229 notes)\n",
    "    * Roughly, start with errors iid ~ N(0, sigma \\*\\* 2). Then the likelihood is a product of probabilities, then maximize it. This will be equivalent to minimizing MSE.\n",
    "\n",
    "5. Describe Lp Norms in the Context of Regularization\n",
    "    * L1 regularization uses L1 norm to compute the size of parameters, and L2 uses L2\n",
    "\n",
    "6. Describe use cases, weaknesses and strengths of Elastic Net Regulariztion\n",
    "    * Use when you have p >> n and want more than n features (as Lasso does), and you don't want to drop all but one of a bunch of correlated features, as LASSO would do.\n",
    "    * Strengths: Reduces to LASSO and ridge, balances against sparsity of Lasso\n",
    "    * Weaknesses: More complex to tune\n",
    "\n",
    "7. Discuss in plain English the \"\"Squid\"\" plot in terms of each of the above three regularization techniques and their respective Lp norm.\n",
    "    * The Squid plot tracks the value of a beta coefficient of a linear model vs strength of regularization. We use it with different regularizations (L1, L2, etc.) to see how fast the coeffients go and stay at 0. We find that L1 regularization tends to send more coeffients to 0, whereas L2 lowers the magnitude but never zeroes any coeffient out.\n",
    "\n",
    "8. Describe use cases, weaknesses and strengths of binary and multiclass SVM Classifiers\n",
    "    * Use cases: classifiers with continuous data that may or may not be linearly separable\n",
    "    * Strengths: performs like logistic regression in linear case, allows use of kernel trick for nonlinear case\n",
    "    * Weaknesses: difficult to code (relies on Quadratic Programming), non interpretable from probabilistic point of view (vs. logistic regression)\n",
    "\n",
    "9.  Write the cost function of SVMs\n",
    "    * $\\Lambda(x,y,\\alpha) = \\sum_{i=0}^{n}\\alpha_{i}-\\frac{1}{2}\\sum_{i=0}^{n}\\sum_{j=0}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}$<br><br>\n",
    "\n",
    "10. Write a description of Decision Trees\n",
    "        def train_decision_tree_clr(partitions, partition_p, X_train, y_train):\n",
    "            stop if meet base case (e.g. pruning condition)\n",
    "            greedily find feature split that minimizes Gini inpurity or Entropy\n",
    "            produce new partition_p, partition_q and update partitions\n",
    "            train_decision_tree_clr(partitions, partition_p, X_train, y_train)\n",
    "            train_decision_tree_clr(partitions, partition_q, X_train, y_train)\n",
    "\n",
    "11. Describe use cases, weaknesses and strengths of Decision Trees\n",
    "    * Use cases: Interpretable nonlinear model for regression or classification\n",
    "    * Strengths: Interpretable nonlinear model\n",
    "    * Weaknesses: tends to overfit\n",
    "\n",
    "12. Write a description of the Random Forest algorithm.\n",
    "        def random_forest(X_train, y_train, n_trees, n_features):\n",
    "            Create n_trees bootstrap samples of X_train, y_train\n",
    "            for each bootstrap sample:\n",
    "                train a decision tree on sample, splitting only choosing from n_features random features\n",
    "            return all trees\n",
    "\n",
    "13. Describe use cases, weaknesses and strengths of Random Forest. \n",
    "    * Use cases: Use for supervised learning with a mix of continuous and discrete features\n",
    "    * Strengths: Robust against missing data, unscaled data, typically reduces variance compared to decision tree\n",
    "    * Weakness: Non interpretable, may require many passes over data\n",
    "\n",
    "14. Discuss Ensemble Classifiers and the Wisdom of the Crowd.\n",
    "    * Ensemble classifiers reduce variance of a single classifer by bootstrap sampling the training set and then by taking most common class among classifiers for a result.\n",
    "\n",
    "15. Describe Bagging and use cases for bagging with standard classifiers.\n",
    "        def bag(k, classifier, X_train, y_train):\n",
    "            result_classifiers = []\n",
    "            for k times:\n",
    "                bootstrapped_train = draw len(X_train) from (X_train, y_train) with replacement\n",
    "                result_classifiers adds classifer trained on bootstrapped_train\n",
    "            return result_classifiers\n",
    "            \n",
    "        def predict(classifiers, X_test):\n",
    "            for obs in X_test:\n",
    "                yield label with most cfr casting it in \n",
    "                    [(cfr, cfr.predict(obs)) for cfr in classifiers]\n",
    "        \n",
    "    * Use cases: Decrease Variance of a high variance classifer\n",
    "\n",
    "16. Describe the basic algorithms for Boosting Forests - Gradient Boosting and Adaptive Boosting\n",
    "        # Start with a (high bias) decision tree. \n",
    "        # Construct a tree that models the residual. Repeat until error is low.    \n",
    "        Initialize weights to be 1/N samples\n",
    "    \n",
    "        for m in M (determined by user):\n",
    "            fit a decision tree\n",
    "            compute total error\n",
    "            compute alpha\n",
    "            chage weights by an exponent if misclassified\n",
    "        \n",
    "        # Predict:\n",
    "        for m in M(determined by user):\n",
    "            return the summation of the predicit of every decision tree multiplied by the weight\n",
    "\n",
    "17. Describe use cases for Boosting Forests - Gradient Boosting and Adaptive Boosting\n",
    "    * Gradient boosting for trees is used when we'd like to start with a high bias decision tree and add more trees to it (using +) to correct the errors of the previous sum of trees in order to lower bias. So it has the same properties of decision trees, just with lower bias.\n",
    "\n",
    "18. Describe the Gradient Boosting Algorithm\n",
    "        # Fit\n",
    "        Create a decision tree and set residuals (r) and assign labels\n",
    "        for m in M (Set by user):\n",
    "            fit a new decision tree on the data trying to predict the residuals\n",
    "            calculate the lagrange multiplier using argmin on the loss function\n",
    "            New tree updates the old tree's  leaves by computing a gradient descent step in the direction of the gradient, using the lagrange multiplier as a weight.\n",
    "            recompute the residuals with the newly calculated tree\n",
    "        \n",
    "        # Predict: \n",
    "        Normal decision tree predict function\n",
    "\n",
    "19. Write a description of the K-means algorithm - Describe use cases, weaknesses and strengths\n",
    "        def k_means(X, k, n_iter):\n",
    "            centers = [0] * k\n",
    "            for n_iter times:\n",
    "                compute all distances between centers and all points\n",
    "                new_centers = []\n",
    "                for each center:\n",
    "                    compute mean of all points closest to center\n",
    "                    add new center to new_centers\n",
    "                centers = new_centers\n",
    "            return centers\n",
    "            \n",
    "    * Use cases: quick and dirty clustering of data\n",
    "    * Strengths: interpretable, easy to code\n",
    "    * Weaknesses: requires setting k in advance, rigid (as assumes clusters are circular, roughly), sensitive to curse of dimensionality\n",
    "\n",
    "20. Discuss the differences and similarities between K-means and K-medoids. What reasons would we have to choose one over the other?\n",
    "    * k-means computes the actual mean of clusters each iteration. k-medoids does not provide the dataset new points to \"center\" clusters around; it designates actual dataset points to be \"centers\" and computes distances toward them. k-medoid is more robust to noise and outliers because it doesn't compute a mean: in this way, it is robust to noise and outliers like the median is compared to mean in 1-dimensional case.\n",
    "\n",
    "21. Write a description of the GMM algorithm - Describe its use cases, weaknesses and strengths\n",
    "    * Use cases: Gaussian mixture\n",
    "    * Strengths: excellent if clusters are not circles, but still ellipses (i.e., Gaussian)\n",
    "    * Weaknesses: doesn't work if not a Gaussian mixture problem.\n",
    "\n",
    "22. Discuss and compare the differences and similarities between GMM and K-means. What reasons would one have to choose one over the other?\n",
    "    * k-means is like a simplified form of GMM: it assumes that clusters are roughly circular, which are Gaussian\n",
    "    * GMM is more general and can capture wider Gaussian mixtures.\n",
    "\n",
    "23. Write a description of the DBSCAN clustering algorithm. Describe use cases, weaknesses and strengths.\n",
    "    * See below for code/pseudocode. DBSCAN is a proximity clustering algorithm. It checks to see if a point is within a certain proximity (epsilon) of other point(s) and then assigns those points to a cluster. If the number of points within the cluster is greater than the minimum number of point (specified by the user), then that becomes a cluster. Otherwise those points are classified as noise.\n",
    "\n",
    "    * DBSCAN is useful for high dimensional data and for situations when the number of clusters is unknown, but if the number of groups is known, DBSCAN may end up coming up with a different number of clusters.\n",
    "\n",
    "24.  Write the equations for and use AIC, BIC, AICc and Cp to assess model quality relative to other models. Discuss the differences and similarity between these measures in detail and the motivational foundations for using one over the other.\n",
    "    * k is the number of coefficients; n is the number of items in a sample; L is the maximized value of the likelihood function of the model; SSE is the sum of squared errors; $S^2$ is the residual mean square\n",
    "\n",
    "    * AIC = $2k - 2ln(L)$\n",
    "\n",
    "    * AICc = $AIC + 2k(k+1)/(n-k-1)$\n",
    "\n",
    "    * BIC = $-2ln(L) + k ln(n)$\n",
    "\n",
    "    * Cp = $SSE/S^2 - n + 2k$\n",
    "\n",
    "    * AIC is a model evaluation technique from information theory. It assumes 2 distributions for the distribution f (assumed to a distribution that generated the data) and calculates the degree to which the distributions match f and the information lost by choosing one over the other to represent f.\n",
    "\n",
    "    * AICc corrects AIC for finite sample sizes by adding a weight for the coefficients and sample size.\n",
    "\n",
    "    * BIC is similiar to AIC, but it can only be used in cases where the same samples are being compared. It penalizes free parameters more strongly than AIC. \n",
    "\n",
    "    * Cp is used to evaulate OLS regression models. It is useful for finding the optimal subset of parameters for a model. \n",
    "\n",
    "25. Describe the standard metrics of description in clustering: Dispersion, Gap and Silhouette.\n",
    "    * Dispersion measures the distance of every point to every other point within a cluster and then returns the summation of the sum of those distances times 1/(2*the number of points in the cluster). \n",
    "\n",
    "    * Silhouette improves upon the Dispersion metric and measures the:\n",
    "        * a(i) average dissimilarity of every point in a cluster to every other point\n",
    "        * b(i) the lowest average dissimilarity of i point to any other cluster\n",
    "    \n",
    "    * Silhouette is highest when a(i) is greater than b(i) and lowest when b(i) is greater than a(i). \n",
    "\n",
    "    * s(i) = (b(i) - a(i)) / max(a(i),b(i))\n",
    "\n",
    "    * The Gap statistic is another way to determine the optimal number of clusters. It utilizes a reference curve to simulate the noise in the data set. The Gap statistic is maximized when the dispersion within the dataset is farthest away from the reference distribution.\n",
    "\n",
    "    * $Gap_{nk} = E*_n {log(W_k)} - log(W_k)$\n",
    "\n",
    "26. Implement and use elbow plot to determine clustering quality\n",
    "    * An elbow plot tracks some type of dispersion (e.g. intracluster distance) vs number of clusters. The kink in the elbow plot tells you roughly that after that number of clusters you'll get very low reduction in dispersion. So you'd do ok if you take the lowest number of clusters after that kink.\n",
    "\n",
    "27. Describe the similarities, differences and use of Gini index and Entropy in the Construction of Decision Trees\n",
    "    * Difference between Gini and Entropy: one is smooth and one is piecewise smooth\n",
    "    * Similarities between Gini and Entropy: roughly, reducing one will reduce the other similarly.\n",
    "    \n",
    "28. Provide the formulas for and discuss Precision, Recall and the F-1 statistic.\n",
    "    * Precision = TP/(TP + FP)\n",
    "    * Recall = TP/(TP + FN)\n",
    "    * F-1 = (2 \\* Precision \\* Recall) / (Precision + Recall)\n",
    "    * Higher precision means fewer false positives, whereas higher recall is fewer false negatives. F-1 is an average of the two.\n",
    "\n",
    "29. What was the point of this class?\n",
    "    * Some Zen shit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Section (60%):\n",
    "\n",
    "Prove that you can code up your own very basic algorithm from simple pseudocode and test it. Write your own version of one of the following two algorithms in python. You may use any resources you can find on the internet with the exception of copying another person's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "\n",
    "    DBSCAN(D, eps, MinPts) {\n",
    "       C = 0\n",
    "       for each point P in dataset D {\n",
    "          if P is visited\n",
    "             continue next point\n",
    "          mark P as visited\n",
    "          NeighborPts = regionQuery(P, eps)\n",
    "          if sizeof(NeighborPts) < MinPts\n",
    "             mark P as NOISE\n",
    "          else {\n",
    "             C = next cluster\n",
    "             expandCluster(P, NeighborPts, C, eps, MinPts)\n",
    "          }\n",
    "       }\n",
    "    }\n",
    "\n",
    "    expandCluster(P, NeighborPts, C, eps, MinPts) {\n",
    "       add P to cluster C\n",
    "       for each point P' in NeighborPts { \n",
    "          if P' is not visited {\n",
    "             mark P' as visited\n",
    "             NeighborPts' = regionQuery(P', eps)\n",
    "             if sizeof(NeighborPts') >= MinPts\n",
    "                NeighborPts = NeighborPts joined with NeighborPts'\n",
    "          }\n",
    "          if P' is not yet member of any cluster\n",
    "             add P' to cluster C\n",
    "       }\n",
    "    }\n",
    "\n",
    "    regionQuery(P, eps)\n",
    "       return all points within P's eps-neighborhood (including P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAM\n",
    "\n",
    "\n",
    "    k_medoids(samples, k = 3){\n",
    "\n",
    "\tstore the number of points in the samples\n",
    "\n",
    "\trandomly select k points as the first medoids - (thus the point label is the \"medoid\" - i.e. points 46 and 48 might belong to the same medoid but 46 is the center, so 46 and 48 will be both labeled as \"46\" meaning that's the current medoid).\n",
    "    \n",
    "\tInitialize Old and New medoids storage\n",
    "\n",
    "\twhile not (oldMedoids == currentMedoids){\n",
    "\t    Assign each point to cluster with closest medoid. (expectation)\n",
    "\n",
    "\t    Update medoids for each cluster. Store to new medoids (maximization)\n",
    "\n",
    "        #this part checks to see if things are still changing\n",
    "\t    Set old medoids = current medoids\n",
    "\t    set current Medoids = newMedoids\n",
    "\n",
    "    }\n",
    "\n",
    "\treturn clusters, currentMedoids\n",
    "\n",
    "    }\n",
    "\n",
    "    expectation(samples, medoids, k):\n",
    "    # assigns each point to the cluster with the closest medoid\n",
    "    make empty array to store clusters (length samples)\n",
    "\n",
    "\tfor sample in samples{\n",
    "        get distances between each sample and each medoid label point for all clusters\n",
    "\t\tassign that sample to the medoid it is closest to\n",
    "    }\n",
    "\treturn clusters\n",
    "\n",
    "    maximization(samples, clusters, k){\n",
    "    # this calculates the best center (new medoid label - which will be a sample label) for a given medoid.\n",
    "\n",
    "    Get a list of all possible distances between points inside the same medoid (likely 2d matrix)\n",
    "    \n",
    "    Swap medoid labels and other members within that cluster until you find the configuration where the sum of distances is the lowest. (there are cheap ways to do this) I.E. see if any other point within the cluster would be a better medoid label. This is the most central point for that cluster and the new medoid. \n",
    "    \n",
    "    Set all labels within that cluster to the new medoid.\n",
    "\n",
    "\treturn medoids\n",
    "    }\n",
    "\n",
    "    getDistance(sample1, sample2, method){\n",
    "\n",
    "        return distance calculated between sample1 and sample2 with a chosen method\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your algorithms have a fit() and a predict() method. Build a test harness to make sure everything works. Use scikits make_classification to set up a few simple clustering problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
