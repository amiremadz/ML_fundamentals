{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 Skills Test 3\n",
    "\n",
    "This skills test is designed to provide a review and cover all remaining material left in the mastery skills tracker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Section (40%):\n",
    "    \n",
    "Short answers are best! \"description\" of an algorithm should be done in pseudocode. Use the pseudocode in the notes as a template.\n",
    "    \n",
    "1. Describe the use cases, weaknesses and strengths of OLS - including Multilinear and Polynomial\n",
    "\n",
    "2. Write the 6 OLS assumptions for BLUE\n",
    "\n",
    "3. Implement OLS in Matrix Form\n",
    "\n",
    "4. Derive and write the cost function of OLS\n",
    "\n",
    "5. Describe Lp Norms in the Context of Regularization\n",
    "\n",
    "6. Describe use cases, weaknesses and strengths of Elastic Net Regulariztion\n",
    "\n",
    "7. Discuss in plain English the \"\"Squid\"\" plot in terms of each of the above three regularization techniques and their respective Lp norm.\n",
    "\n",
    "8. Describe use cases, weaknesses and strengths of binary and multiclass SVM Classifiers\n",
    "\n",
    "9.  Write the cost function of SVMs\n",
    "\n",
    "10. Write a description of Decision Trees\n",
    "\n",
    "11. Describe use cases, weaknesses and strengths of Decision Trees\n",
    "\n",
    "12. Write a description of the Random Forest algorithm.\n",
    "\n",
    "13. Describe use cases, weaknesses and strengths of Random Forest. What is the difference between the Gini and Entropy measures? What are their similarities?\n",
    "\n",
    "14. Discuss Ensemble Classifiers and the Wisdom of the Crowd.\n",
    "\n",
    "15. Describe Bagging and use cases for bagging with standard classifiers.\n",
    "\n",
    "16. Describe the basic algorithms for Boosting Forests - Gradient Boosting and Adaptive Boosting\n",
    "\n",
    "17. Describe use cases for Boosting Forests - Gradient Boosting and Adaptive Boosting\n",
    "\n",
    "18. Describe the Gradient Boosting Algorithm\n",
    "\n",
    "19. Write a description of the K-means algorithm - Describe use cases, weaknesses and strengths\n",
    "\n",
    "20. Discuss the differences and similarities between K-means and K-medoids. What reasons would we have to choose one over the other?\n",
    "\n",
    "21. Write a description of the GMM algorithm - Describe its use cases, weaknesses and strengths\n",
    "\n",
    "22. Discuss and compare the differences and similarities between GMM and K-means. What reasons would one have to choose one over the other?\n",
    "\n",
    "23. Write a description of the DBSCAN clustering algorithm. Describe use cases, weaknesses and strengths.\n",
    "\n",
    "24.  Write the equations for and use AIC, BIC, AICc and Cp to assess model quality relative to other models. Discuss the differences and similarity between these measures in detail and the motivational foundations for using one over the other.\n",
    "\n",
    "25. Describe the standard metrics of description in clustering: Dispersion, Gap and Silhouette.\n",
    "\n",
    "26. Implement and use elbow plot to determine clustering quality\n",
    "\n",
    "27. Describe the similarities, differences and use of Gini index and Entropy in the Construction of Decision Trees\n",
    "\n",
    "28. Provide the formulas for and discuss Precision, Recall and the F-1 statistic.\n",
    "\n",
    "29. What was the point of this class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Section (60%):\n",
    "\n",
    "Prove that you can code up your own very basic algorithm from simple pseudocode and test it. Write your own version of one of the following two algorithms in python. You may use any resources you can find on the internet with the exception of copying another person's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "\n",
    "    DBSCAN(D, eps, MinPts) {\n",
    "       C = 0\n",
    "       for each point P in dataset D {\n",
    "          if P is visited\n",
    "             continue next point\n",
    "          mark P as visited\n",
    "          NeighborPts = regionQuery(P, eps)\n",
    "          if sizeof(NeighborPts) < MinPts\n",
    "             mark P as NOISE\n",
    "          else {\n",
    "             C = next cluster\n",
    "             expandCluster(P, NeighborPts, C, eps, MinPts)\n",
    "          }\n",
    "       }\n",
    "    }\n",
    "\n",
    "    expandCluster(P, NeighborPts, C, eps, MinPts) {\n",
    "       add P to cluster C\n",
    "       for each point P' in NeighborPts { \n",
    "          if P' is not visited {\n",
    "             mark P' as visited\n",
    "             NeighborPts' = regionQuery(P', eps)\n",
    "             if sizeof(NeighborPts') >= MinPts\n",
    "                NeighborPts = NeighborPts joined with NeighborPts'\n",
    "          }\n",
    "          if P' is not yet member of any cluster\n",
    "             add P' to cluster C\n",
    "       }\n",
    "    }\n",
    "\n",
    "    regionQuery(P, eps)\n",
    "       return all points within P's eps-neighborhood (including P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAM\n",
    "\n",
    "\n",
    "    k_medoids(samples, k = 3){\n",
    "\n",
    "\tstore the number of points in the samples\n",
    "\n",
    "\trandomly select k points as the first medoids - (thus the point label is the \"medoid\" - i.e. points 46 and 48 might belong to the same medoid but 46 is the center, so 46 and 48 will be both labeled as \"46\" meaning that's the current medoid).\n",
    "    \n",
    "\tInitialize Old and New medoids storage\n",
    "\n",
    "\twhile not (oldMedoids == currentMedoids){\n",
    "\t    Assign each point to cluster with closest medoid. (expectation)\n",
    "\n",
    "\t    Update medoids for each cluster. Store to new medoids (maximization)\n",
    "\n",
    "        #this part checks to see if things are still changing\n",
    "\t    Set old medoids = current medoids\n",
    "\t    set current Medoids = newMedoids\n",
    "\n",
    "    }\n",
    "\n",
    "\treturn clusters, currentMedoids\n",
    "\n",
    "    }\n",
    "\n",
    "    expectation(samples, medoids, k):\n",
    "    # assigns each point to the cluster with the closest medoid\n",
    "    make empty array to store clusters (length samples)\n",
    "\n",
    "\tfor sample in samples{\n",
    "        get distances between each sample and each medoid label point for all clusters\n",
    "\t\tassign that sample to the medoid it is closest to\n",
    "    }\n",
    "\treturn clusters\n",
    "\n",
    "    maximization(samples, clusters, k){\n",
    "    # this calculates the best center (new medoid label - which will be a sample label) for a given medoid.\n",
    "\n",
    "    Get a list of all possible distances between points inside the same medoid (likely 2d matrix)\n",
    "    \n",
    "    Swap medoid labels and other members within that cluster until you find the configuration where the sum of distances is the lowest. (there are cheap ways to do this) I.E. see if any other point within the cluster would be a better medoid label. This is the most central point for that cluster and the new medoid. \n",
    "    \n",
    "    Set all labels within that cluster to the new medoid.\n",
    "\n",
    "\treturn medoids\n",
    "    }\n",
    "\n",
    "    getDistance(sample1, sample2, method){\n",
    "\n",
    "        return distance calculated between sample1 and sample2 with a chosen method\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your algorithms have a fit() and a predict() method. Build a test harness to make sure everything works. Use scikits make_classification to set up a few simple clustering problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
