{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 7.2 Lecture\n",
    "\n",
    "## A Guide to Basic Feature Selection\n",
    "\n",
    "\n",
    "### By the End of This Lecture You Will Be:\n",
    "1. Familiar with the principles of feature construction\n",
    "2. Familiar with Forward Selection\n",
    "3. Familiar with Backward Selection\n",
    "4. Familiar with Recursive Feature Elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is intended to select the “best” subset of predictors. But why bother?\n",
    "\n",
    "1. We want to explain the data in the simplest way — redundant predictors (features) should be removed. Simplest is always best. (QUIZ: Why? Robustness)\n",
    "2. Unnecessary predictors (features) will add noise terms to the estimation of other quantities that we are interested in. Degrees of freedom (affects error estimates) will be wasted. \n",
    "3. Collinearity is caused by having too many variables trying to do the same job.\n",
    "4. Cost: if the model is to be used for prediction, we can save time and/or money by avoiding measurement of redundant predictors in future experiments.\n",
    "\n",
    "Prior to feature selection, during EDA:\n",
    "\n",
    "1. Identify outliers and influential points - maybe exclude them at least temporarily.\n",
    "2. Add in any transformations of the variables that seem appropriate. This may take a while - in particular look for transformations that reproduce linearity when projected against other variables.\n",
    "\n",
    "###QUIZ: \n",
    "How do you determine appropriateness of a transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nature of features - statistical relationships are not always obvious to the eye\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Features can be meaningless alone, but informative together.\n",
    "\n",
    "![f1](./images/Features_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Sometimes you need both highly correlated features to properly distinguish classes.\n",
    "\n",
    "![f2](./images/Features_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sometimes highly relevant features are useless.\n",
    "\n",
    "![f3](./images/Features_3a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hierarchy\n",
    "\n",
    "Some models have a natural hierarchy. For example, in polynomial models, $x^2$ is a term of higher order than x. You must respect this hierarchy. Higher order terms should be removed from the model before lower order terms in the same variable, due to the multiplication of effect from the error terms. if \n",
    "\n",
    "$$\\tilde{x} = x_{observed}+\\epsilon$$\n",
    "\n",
    "Then the placement of this observable in the higher order terms propagates the error term directly. If the model includes polynomial terms, $y = \\beta_{0}+\\beta_{1}x+\\beta_{2}x^2$, then the estimated y includes squared error terms:\n",
    "\n",
    "$$\\hat{y} = \\beta_{0}+\\beta_{1}(x_{observed}+\\epsilon)+\\beta_{2}(x_{observed}+\\epsilon)^2$$\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\beta_{0}+\\beta_{1}(x_{observed}+\\epsilon)+\\beta_{2}(x_{observed}^2+2\\ x_{observed}\\ \\epsilon+\\epsilon^2)$$\n",
    "\n",
    "Which is a problem. Now the first order term in $x$ has reappeared, but without a corresponding weight to fit on.\n",
    "\n",
    "Suppose you remove the first order term and then make a scale change $x \\rightarrow x+a$, the same problem appears:\n",
    "\n",
    "$$\\hat{y} = \\beta_{0}+\\beta_{2}(x_{observed}^2+2\\ x_{observed}\\ a+a^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms and Cross Terms\n",
    "\n",
    "Suppose we produce a second-order model with a response surface as follows:\n",
    "\n",
    "$$\\hat{y} = \\beta_{0}+\\beta_{1}(x_{1})+\\beta_{2}(x_{2})+\\beta_{11}(x_{1})^2+\\beta_{22}(x_{2})^2+\\beta_{12}x_{1}\\ x_{2}$$\n",
    "\n",
    "We would not normally consider removing the $x_{1}x_{2}$ interaction term without considering the simultaneous removal of the x^2 terms. A joint removal would correspond to the clearly meaningful comparison of a quadratic surface and linear one. Just removing the  $x_{1}x_{2}$ term would correspond to a surface that is aligned with the coordinate axes.\n",
    "\n",
    "Such a move would be difficult to interpret and would not be done unless some meaning can be attached. Any rotation of the feature space would reintroduce the interaction term (as above).\n",
    "\n",
    "### In general: Effect Heredity\n",
    "\n",
    "The effect heredity principle, indicates that an interaction can be active\n",
    "only if one or both of its parent effects are also active. For example,\n",
    "a two-factor interaction can be active only if one or both of the corresponding main effects are active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels - Looking at OLS outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 4.020e+06\n",
      "Date:                Wed, 09 Dec 2015   Prob (F-statistic):          2.83e-239\n",
      "Time:                        11:56:07   Log-Likelihood:                -146.51\n",
      "No. Observations:                 100   AIC:                             299.0\n",
      "Df Residuals:                      97   BIC:                             306.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.3423      0.313      4.292      0.000         0.722     1.963\n",
      "x1            -0.0402      0.145     -0.278      0.781        -0.327     0.247\n",
      "x2            10.0103      0.014    715.745      0.000         9.982    10.038\n",
      "==============================================================================\n",
      "Omnibus:                        2.042   Durbin-Watson:                   2.274\n",
      "Prob(Omnibus):                  0.360   Jarque-Bera (JB):                1.875\n",
      "Skew:                           0.234   Prob(JB):                        0.392\n",
      "Kurtosis:                       2.519   Cond. No.                         144.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "np.random.seed(9876789)\n",
    "\n",
    "###Artifical Data####\n",
    "nsample = 100\n",
    "x = np.linspace(0, 10, 100)\n",
    "X = np.column_stack((x, x**2))\n",
    "beta = np.array([1, 0.1, 10])\n",
    "e = np.random.normal(size=nsample)\n",
    "\n",
    "####Add intercept####\n",
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + e\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Selection\n",
    "\n",
    "\n",
    "1. Start with all the predictors in the model\n",
    "2. Remove the predictor with highest p-value greater than αcrit\n",
    "3. Refit the model and goto 2\n",
    "4. Stop when all p-values are less than $\\alpha_{crit}$\n",
    "\n",
    "The $\\alpha_{crit}$ is sometimes called the “p-to-remove” and does not have to be 5%. If prediction performance is the goal, then a 15-20% cut-off may work best, although methods designed more directly for optimal\n",
    "prediction should be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BS_1](./images/BS_1.png)\n",
    "![BS_1_b](./images/BS_1_bottom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we remove the largest p-value violator first (Area):\n",
    "![BS_2](./images/BS_2a.png)\n",
    "\n",
    "Then we can remove the the other two in sequence:\n",
    "\n",
    "![BS_3](./images/BS_3a.png)\n",
    "\n",
    "![BS_4](./images/BS_4a.png)\n",
    "\n",
    "We end up getting a robust model with similar predictivity:\n",
    "\n",
    "![BS_5_top](./images/BS_5_top.png)\n",
    "![BS_5_main](./images/BS_5_main_a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Selection\n",
    "\n",
    "1. Start with no variables in the model.\n",
    "2. For all predictors not in the model, check their p-value if they are added to the model. Choose the one with lowest p-value less than $\\alpha_{crit}$\n",
    "3. Continue until no new predictors can be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "\n",
    "This is a combination of backward elimination and forward selection. This addresses the situation where variables are added or removed early in the process and we want to change our mind about them later. At each stage a variable may be added or removed. There are several variations on exactly how this is done, typically with automated methods of generating combinations. The version implemented in scikit is recursive feature elimination, which does a recursive search for adding and subtracting features along a chosen metric (as a tree). \n",
    "\n",
    "Stepwise procedures are relatively cheap computationally but they do have some drawbacks.\n",
    "\n",
    "1. Because of the “one-at-a-time” nature of adding/dropping variables, it’s possible to miss the “optimal” model.\n",
    "2. The p-values used should not be treated too literally. The removal of less significant predictors tends to increase the significance of the remaining predictors. This effect leads one to overstate the importance of the remaining predictors.\n",
    "3. The procedures are not directly linked to final objectives of prediction or explanation and so may notreally help solve the problem of interest. Feature selection tends to amplify the statistical significance of the variables that stay in the model. Features that are dropped can still be correlated with the response.\n",
    "4. Stepwise feature selection tends to pick models that are smaller than desirable for prediction purposes. To give a simple example, consider the simple regression with just one predictor variable. Suppose that the slope for this predictor is not quite statistically significant. We might not have enough evidence to say that it is related to y but it might be better to use it than not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection by Lasso (Elastic net)\n",
    "\n",
    "You have already learned feature selection through regularization procedures. This can be a powerful alternative to classical feature selection; you will need to construct the squid plot (solution path plot) as outlined. Relationships amongst variables can be examined by eye, but careful analysis of the meaning of the features is required to determine which of these are most useful for a final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection by Feature Importance\n",
    "\n",
    "Feature importance with random forests is a great way to tease out the most predictive features, however one must be careful of the heredity effect. Dropping a feature with a parental relationship with a more predictive feature is eliminating dependency structure in the model that is likely necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
