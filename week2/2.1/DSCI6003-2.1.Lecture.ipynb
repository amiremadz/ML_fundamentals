{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cross Validating Regression\n",
    "\n",
    "\n",
    "##After this Lecture you Should Be Able To:\n",
    "Discuss the following topics and know how to implement them in sklearn\n",
    "1. [Cross Validation](#cross-validation)\n",
    "1. [Regression with sklearn](#regression-with-sklearn)\n",
    "1. [Fitting a polynomial](#fitting-a-polynomial)\n",
    "\n",
    "You should also be familiar with the standard graphs for these issues and be able to explain them and understand how they affect your own work.\n",
    "1. [Bias and Variance](#bias-and-variance)\n",
    "1. [Overfitting and Underfitting](#overfitting-and-underfitting)\n",
    "\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "Before you begin fitting a model on a new dataset you should, almost always, split your initial dataset into a \"train\" dataset and a \"test\" dataset. You will use the train dataset to build your model and the test dataset to measure the model's success.\n",
    "\n",
    "You should generally keep 10-50% of the data for the test set and use the rest for training. \n",
    "\n",
    "You should always split your data as randomly as possible. It should be obvious to you that the slightest inclusion of a nonrandom process in the selection of the training set can skew model parameters. Data is frequently sorted in some way (by date or even by the value you are trying to predict).\n",
    "\n",
    "*Never* just split your data into the first 90% and the remaining 10%. Lucky for us, there is a nice method implemented in scipy that splits the dataset randomly for us called[test_train_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html). If you do not happen to have this tool in the language you're working in (i.e. Java), you'll need to make it.\n",
    "\n",
    "\n",
    "### KFold Cross Validation\n",
    "\n",
    "One thing about doing standard cross validation is that, as discussed above, your score can depend on the random split. We can get a more accurate value of the error by using *KFold Cross Validation*. \n",
    "\n",
    "Basically, we break the data into k groups. We take one of these groups and make it the test set. We then train the data on the remaining groups and calculate the error on the test group. We can repeat this process k times and average all the results. This gives us a more accurate picture of the error.\n",
    "\n",
    "## Regression with sklearn\n",
    "\n",
    "There are several good modules with implementations of regression. We've\n",
    "used\n",
    "[statsmodels](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html).\n",
    "Today we will be using [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "[numpy](docs.­scipy.­org/­doc/­numpy/­reference/­generated/­numpy.­polyfit.­html) and\n",
    "[scipy](http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.linregress.html)\n",
    "also have implmentations.\n",
    "\n",
    "Resources:\n",
    "* [sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* [sklearn example](http://scikit-learn.org/0.11/auto_examples/linear_model/plot_ols.html)\n",
    "\n",
    "For all `sklearn` modules, the `fit` method is used to train and the `score`\n",
    "method is used to test. You can also use the `predict` method to see the\n",
    "predicted y values.\n",
    "\n",
    "#### Example\n",
    "\n",
    "This is the general workflow for working with sklearn. Any algorithm we use from sklearn will have the same form.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load data from csv file\n",
    "df = pd.read_csv('data/housing_prices.csv')\n",
    "X = df[['square_feet', 'num_rooms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Run Linear Regression\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print \"Intercept:\", regr.intercept_\n",
    "print \"Coefficients:\", regr.coef_\n",
    "print \"R^2 error:\", regr.score(X_test, y_test)\n",
    "predicted_y = regr.predict(X_test)\n",
    "```\n",
    "\n",
    " `LinearRegression` is a class and you have to create an instance of it. If there are any parameters to the model, you should set them when you instantiate the object. For example, with `LinearRegression`, you can choose whether to normalize you data:\n",
    "\n",
    " ```python\n",
    " regr = LinearRegression(normalize=True)    # the default is False\n",
    " ```\n",
    "\n",
    "You should call the `fit` method once. Here you give it the training data and it will train your model. Once you have that, you can get the coefficients for the equation (`intercept_` and `coef_`) and also get the score for your test set (`score` method). You can also get the predicted values for any new data you would like to give to the model (`predict` method).\n",
    "\n",
    "Here's an example using kfold:\n",
    "\n",
    "```python\n",
    "from sklearn import cross_validation\n",
    "kf = cross_validation.KFold(X.shape[0], n_folds=5, shuffle=True)\n",
    "results = []\n",
    "for train_index, test_index in kf:\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X[train_index], y[train_index])\n",
    "    results.append(regr.score(X[test_index], y[test_index]))\n",
    "print \"average score:\", np.mean(results)\n",
    "```\n",
    "\n",
    "\n",
    "## Fitting a polynomial\n",
    "\n",
    "Oftentimes you'll notice that you're data isn't linear and that really you should be fitting a higher degree polynomial to the line. This is called *underfitting*, which we'll get to later.\n",
    "\n",
    "![quadratic](images/quadratic.png)\n",
    "\n",
    "So how do we do this? We can use the same algorithm, we just need to modify our features. Let's look at the one feature world for a minute. We have data that looks something like this:\n",
    "\n",
    "|     x |     y |\n",
    "| ----- | ----- |\n",
    "|     3 |     8 |\n",
    "|     4 |    17 |\n",
    "|     7 |    40 |\n",
    "|     9 |    78 |\n",
    "|    11 |   109 |\n",
    "\n",
    " For linear regression, we are trying to find the `b` and `c` that minimize the error in the following equation:\n",
    "\n",
    "    bx + c = y\n",
    "\n",
    "To do a *quadratic* regression instead of a *linear* regression, we instead want to find the optimal `a`, `b` and `c` in this equation:\n",
    "\n",
    "    ax^2 + bx + c = y\n",
    "\n",
    "We can just add a new feature to our feature matrix by computing `x^2`:\n",
    "\n",
    "|     x |   x^2 |    y |\n",
    "| ----- | ----- | ----- |\n",
    "|     3 |     9 |     8 |\n",
    "|     4 |    16 |    17 |\n",
    "|     7 |    49 |    40 |\n",
    "|     9 |    81 |    78 |\n",
    "|    11 |   121 |   109 |\n",
    "\n",
    "Now you can do linear regression with these features. If there's a linear relationship between `x^2` and `y` that means there's a quadratic relationship between `x` and `y`.\n",
    "\n",
    "If you have more than one feature, you need to do all the combinations. If you start with two features, `x` and `z`, to do the order 2 polynomial, you will need to add these features: `x^2`, `z^2`, `xz`.\n",
    "\n",
    "In `sklearn`, you should use `PolynomialFeatures` for generating these additional features ([documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)). Here's how you would modify the above example to include polynomial features up to degree 3:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load data is identical as above\n",
    "df = pd.read_csv('data/housing_prices.csv')\n",
    "X = df[['square_feet', 'num_rooms']]\n",
    "y = df['price']\n",
    "\n",
    "# Add the polynomial features\n",
    "poly = PolynomialFeatures(3)\n",
    "X_new = poly.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.15)\n",
    "\n",
    "# Run Linear Regression (the same as above)\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print \"Intercept:\", regr.intercept_\n",
    "print \"Coefficients:\", regr.coef_\n",
    "print \"R^2 error:\", regr.score(X_test, y_test)\n",
    "predicted_y = regr.predict(X_test)\n",
    "```\n",
    "\n",
    "Here's sklearn's [example](http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html). They use a `pipeline` to make the code a little simpler (with the same functionality).\n",
    "\n",
    "In `numpy`, you can use the `polyfit` function ([documentation](http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)).\n",
    "\n",
    "The discussion below will give insight into how to determine which degree of polynomial to pick.\n",
    "\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "We first want to discuss two key terms: *bias* and *variance*.\n",
    "\n",
    "#### Error due to bias\n",
    "Imagine that you construct your model with the same process several hundred times. A *biased* model would center around the incorrect solution. How you collect data can lead to bias (for example, you only get user data from San Francisco and try to use your model for your whole userbase).\n",
    "\n",
    "#### Error due to variance\n",
    "Again, imagine that you construct your model several hundred times. A model with high *variance* would have wildly different results each time. The main contributor to high variance is insufficient data or that what you're trying to predict isn't actually correlated to your features.\n",
    "\n",
    "To see the bais variance tradeoff visually:\n",
    "\n",
    "![bias variance](images/bias_variance.png)\n",
    "\n",
    "Note that both high bias or high variance are bad. Note that high variance is worse than it sounds since you will only be constructing the model once, so with high variance there's a low probability that your model will be near the optimal one.\n",
    "\n",
    "\n",
    "## Overfitting and underfitting\n",
    "\n",
    "Let's get back to fitting the polynomial. Take a look at the following example with three potential curves fit to the data.\n",
    "\n",
    "![underfitting overfitting](images/underfitting_overfitting_1.png)\n",
    "\n",
    "In the first graph, we've fit a line to the data. It clearly doesn't fully represent the data. This is called *underfitting*. This represents high *bias* since the error will be consistently high. It also represents low *variance* since with new data, we would still construct approximately the same model.\n",
    "\n",
    "In the third graph, we've fit a polynomial of high degree to the data. Notice how it accurately gets every point but we'd say this does not accurately represent the data. This is called *overfitting*. This represents high *variance*. If we got new data, we would construct a wildly different model. This also represents low *bias* since the error is very low.\n",
    "\n",
    "The one in the middle is the optimal choice.\n",
    "\n",
    "You can see this graphically:\n",
    "\n",
    "![bias variance](images/bias_variance_graph.png)\n",
    "\n",
    "Model complexity in our case is the degree of the polynomial.\n",
    "\n",
    "Another way of viewing this is by comparing the error on the training set with the error on the test set. When you fit a model, it minimizes the error on the training set. An overfit model can reduce this to 0. However, what we really care about it how well it does on a new test set, since we want our model to perform well on unseen data. This paradigm is represented with the following graph.\n",
    "\n",
    "![overfitting underfitting](images/graph_overfitting_underfitting.png)\n",
    "\n",
    "You can see on the left side of the graph that the data is *underfit*: both the train error and the test error are high. On the right hand side of the graph, the data is *overfit*: the train error is low but the test error is high. The best model is where the train error is lowest, around a degree 5 polynomial.\n",
    "\n",
    "\n",
    "## Regularization: How to deal with overfitting\n",
    "\n",
    "Having lots of features can lead to overfitting. When we make it into a polynomial, we are adding *a lot* of features. *Regularization* is a way of penalizing coefficients for being large.\n",
    "\n",
    "## L1 regularization [Lasso regression](http://statweb.stanford.edu/~tibs/lasso/simple.html)\n",
    "Math description: Regularization adds a random variable (for L1, a Laplacian) to the hat (estimate) matrix so that it can be inverted.\n",
    "\n",
    "  1. Turns most regressors to zeros\n",
    "  2. Uses a Laplacian prior\n",
    "\n",
    "### When to use: \n",
    "\n",
    "  1. Large sparse data sets, many regressors will become zero. \n",
    "  2. When you have many regressors but are unsure of the important ones.\n",
    "\n",
    "<u>Pros</u>:\n",
    "\n",
    "  1. Good for recovering sparse datasets\n",
    "  2. Reduce overfitting\n",
    "\n",
    "<u>Cons</u>:\n",
    "\n",
    "  1. More difficult to interpret\n",
    "  2. Loss of predictive power\n",
    "  3. Large estimation error for non-sparse data.\n",
    "\n",
    "## L2 regularization (Ridge regression):\n",
    "1. Ridge regression suppresses the influence of the leading regressors lightly and the lagging regressors  heavily. \n",
    "2. Uses a Gaussian prior\n",
    "\n",
    "### When to use: \n",
    "  1. When you have many regressors but are unsure of the important ones\n",
    "  2. Non-sparse data. \n",
    "\n",
    "<u>Pros</u>:\n",
    "  1. Good for recovering non-sparse signals. \n",
    "  2. Reduce overfitting.\n",
    "  3. Less variance than the OLS estimator [reference](http://tamino.wordpress.com/2011/02/12/ridge-regression/)\n",
    "\n",
    "<u>Cons</u>:\n",
    "\n",
    "  1. The new estimates of the regressors are lower than the OLS estimates [reference](http://tamino.wordpress.com/2011/02/12/ridge-regression/)\n",
    "  2. Loss of predictive power\n",
    "\n",
    "## <u>More references</u>:\n",
    "\n",
    "* [Ridge regression](http://tamino.wordpress.com/2011/02/12/ridge-regression/)\n",
    "* [Lasso regression](http://statweb.stanford.edu/~tibs/lasso/simple.html)\n",
    "* [Difference between L1 and L2](http://www.quora.com/Machine-Learning/What-is-the-difference-between-L1-and-L2-regularization), Aleks Jakulins answer. \n",
    "* [Matrix for of regression models](http://global.oup.com/booksites/content/0199268010/samplesec3)\n",
    "* [The statistics bible](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf), chapter 3\n",
    "* [stats.stackexchange: Ridge vs. LASSO](http://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge)\n",
    "* [MetaOptimize: L1 vs. L2](http://metaoptimize.com/qa/questions/5205/when-to-use-l1-regularization-and-when-l2)\n",
    "\n",
    "## Classification\n",
    "\n",
    "We will cover these concepts in more depth, but there are some basics to understand around how to evaluate a classifier.\n",
    "\n",
    "### Measuring success\n",
    "\n",
    "So how do we measure how well our model does? Just like with regression, we need to split the data in a training set and a test set and measure our success based on how well it does on the test set.\n",
    "\n",
    "#### Accuracy\n",
    "The simplest measure is **accuracy**. This is the number of correct predictions over the total number of predictions. It's the percent you predicted correctly. In `sklearn`, this is what the `score` method calculates.\n",
    "\n",
    "#### Shortcomings of Accuracy\n",
    "Accuracy is often a good first glance measure, but it has many shortcomings. If the classes are unbalanced, accuracy will not measure how well you did at predicting. Say you are trying to predict whether or not an email is spam. Only 2% of emails are in fact spam emails. You could get 98% accuracy by always predicting not spam. This is a great accuracy but a horrible model!\n",
    "\n",
    "#### Confusion Matrix\n",
    "We can get a better picture our model but looking at the confusion matrix. We get the following four metrics:\n",
    "\n",
    "* **True Positives (TP)**: Correct positive predictions\n",
    "* **False Positives (FP)**: Incorrect positive predictions (false alarm)\n",
    "* **True Negatives (TN)**: Correct negative predictions\n",
    "* **False Negatives (FN)**: Incorrect negative predictions (a miss)\n",
    "\n",
    "|            | Predicted Yes  | Predicted No   |\n",
    "| ---------- | -------------- | -------------- |\n",
    "| Actual Yes | True positive  | False negative |\n",
    "| Actual No  | False positive | True negative  |\n",
    "\n",
    "\n",
    "#### Precision, Recall and F1\n",
    "Instead of accuracy, there are some other scores we can calculate:\n",
    "\n",
    "* **Precision**: A measure of how good your positive predictions are\n",
    "    ```\n",
    "    Precison = TP / (TP + FP)\n",
    "             = TP / (predicted yes)\n",
    "    ```\n",
    "* **Recall**: A measure of how well you predict positive cases. Aka *sensitivity*.\n",
    "    ```\n",
    "    Recall = TP / (TP + FN) \n",
    "           = TP / (actual yes)\n",
    "    ```\n",
    "* **F1 Score**: The harmonic mean of Precision and Recall\n",
    "    ```\n",
    "    F1 = 2 / (1/Precision + 1/Recall)\n",
    "       = 2 * Precision * Recall / (Precision + Recall)\n",
    "       = 2TP / (2TP + FN + FP)\n",
    "    ```\n",
    "\n",
    "Accuracy can also be written in this notation:\n",
    "    ```\n",
    "    Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
