## Part 1: Test-train split Cross Validation

This morning you will learn how to use cross validation to evaluate your model.
The goal for this morning is not to try to build a perfect model, as you
tried to do yesterday. The goal is to evaluate the model given some metric you are
interested in.


1. Include the following lines to import the libraries needed:

   ```python
   from sklearn.linear_model import LinearRegression
   from sklearn.cross_validation import KFold
   from sklearn.cross_validation import train_test_split
   from sklearn.cross_validation import cross_val_score
   import numpy as np
   from sklearn.datasets import load_boston
   ```

2. Load in the boston data with the following commands.

   ```python
   boston = load_boston()
   features = boston.data
   target = boston.target # housing price
   ```

   Descriptions for each column in `features`:

   ```
   Attribute Information (in order):
    - CRIM     per capita crime rate by town
    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
    - INDUS    proportion of non-retail business acres per town
    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    - NOX      nitric oxides concentration (parts per 10 million)
    - RM       average number of rooms per dwelling
    - AGE      proportion of owner-occupied units built prior to 1940
    - DIS      weighted distances to five Boston employment centres
    - RAD      index of accessibility to radial highways
    - TAX      full-value property-tax rate per $10,000
    - PTRATIO  pupil-teacher ratio by town
    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
    - LSTAT    % lower status of the population
    - MEDV     Median value of owner-occupied homes in $1000's
   ```

3. Use `train_test_split()` in scikit learn to make a training and test dataset.
   There's no rule on train-test splits.  Below we use a 70 : 30 split.

   ```python
   train_feature, test_feature, train_target, test_target = \
   train_test_split(features, target, test_size=0.3)
   ```

3. Use `LinearRegression()` in scikit-learn to build a model which uses the
   `features` to predict `target`. Only fit the model using the training data set.

   Note that there is multicollinearity and other issues in the data.  Don't worry
   about this for now. We will learn about Lasso and Ridge regularization later today (alternative to the methods you have learned yesterday) to
   deal with some of the issues.

   ```python
   linear = LinearRegression()
   linear.fit(train_feature, train_target)
   # You can call predict to get the predicted values for training and test
   train_predicted = linear.predict(train_feature)
   test_predicted = linear.predict(test_feature)
   ```

4. Write a function that takes the `target` and the `predicted` and calculate
   the **MSE for the training data and the test data**. Use
   `sklearn.metrics.mean_squared_error()` to confirm your results.

   Which did you expect to be higher?

5. Explain the value of evaluating MSE on a separate test set (instead of fitting a
   model and calculating MSE on the entire data set).

6. Make a plot of the MSE as you increase the complexity of the degree of polynomial.  You can create a larger degree polynomial in `scikit-learn` by using the `PolynomialFeatures` model in the preprocessing library
  ```python
   import pandas as pd
   from sklearn.preprocessing import PolynomialFeatures
   from sklearn.linear_model import LinearRegression
   from sklearn.cross_validation import train_test_split

   # Load data is identical as above
   df = pd.read_csv('data/housing_prices.csv')
   X = df[['square_feet', 'num_rooms']]
   y = df['price']

   # Add the polynomial features
   poly = PolynomialFeatures(3)
   X_new = poly.fit_transform(X)
   
   ...
  ```

## Part 2: K-fold Cross Validation

In K-fold cross validation, the data is split into **k** groups. One group
out of the k groups will be the test set, the rest (**k-1**) groups will
be the training set. In the next iteration, another group will be the test set,
and the rest will be the training set. The process repeats for k iterations (k-fold).
In each fold, a metric for accuracy (MSE in this case) will be calculated and
an overall average of that metric will be calculated over k-folds.

<div align="center">
    <img height="300" src="images/kfold.png">
</div>

<br>

Here we will implement K-fold validation **on the training dataset**.
`sklearn` has its own implementation of K-fold
(`sklearn.cross_validation_cross_val_score()`).
However, to ensure you have an understanding of K-fold, you will implement it
here.

<br>

1. To do this you need to manage randomly sampling **k** folds.

2. Properly combining those **k** folds into a test and training set on
   your **on the training dataset**. Outside of the k-fold, there should be
   another set which will be referred to as the **hold-out set**.

3. Train your model on your constructed training set and evaluate on the given test set

3. Repeat steps __2__ and __3__ _k_ times.

4. Average your results of your error metric.

5. Compare the MSE for your test set in Part 1. and your K-fold cross validated error in `4.`.

6. Plot a learning curve and test vs training error curve.
   (You might want to use: [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html) which is scikit-learn's built-in
   function for K-fold cross validation).  See [Illustration of Learning Curves](http://www.astro.washington.edu/users/vanderplas/Astr599/notebooks/18_IntermediateSklearn) for more details.  
