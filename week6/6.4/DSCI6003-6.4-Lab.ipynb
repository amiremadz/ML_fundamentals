{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "For this lab we will build the K-means algorithm from scratch, and test it on the famous [Fisher's Iris](http://archive.ics.uci.edu/ml/datasets/Iris) dataset.  \n",
    "\n",
    "#### The four steps to k-means\n",
    "\n",
    "1. Initialize your cluster centroids\n",
    "2. Compute the distance between each point and every centroid\n",
    "3. Assign each data point to the centroid closest to it\n",
    "4. Move the cluster centroid to the center (mean) of all the points assigned to it\n",
    "5. Repeat until convergence (or you get tired, i.e. hit a maximum number of iterations)\n",
    "\n",
    "![kmean](images/kmeans.gif)\n",
    " (refresh/click to see animation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Now we will implement our  [k-means](http://en.wikipedia.org/wiki/K-means_clustering) clustering.  We will write a k-means algorithm that takes as input a matrix of feature vectors, a k value, and a number of max iterations. \n",
    "\n",
    "We will leave many design decisions up to you, but be sure to write clean, well encapsulated code.  I would suggest either an object oriented approach using a `Kmeans` class or a functional approach where you pass a dataset to a function which runs the iteration for you and spits out the relevant centroids and assignments when your algorithm has finished.\n",
    "\n",
    "1) Load the dataset with `sklearn.datasets.load_iris()`, but since we will be hand coding our K-means in `numpy` we only need to get the features into an array.  Create a numpy array of the features of the iris dataset.  Do not use the labels for the clustering.\n",
    "\n",
    "2) Using Numpy, initialize your cluster centers by selecting random data points.  We will try our algorithm with multiple different `k` but let us start with 10.  Pick at random 10 of our initial points.  May I suggest: [http://docs.python.org/2/library/random.html#random.sample](http://docs.python.org/2/library/random.html#random.sample)\n",
    "\n",
    "3) For each one of your data points, compute the Euclidean distance between it and every centroid. Assign the point to the closest centroid.\n",
    "\n",
    "4) Move the centroids to the center (mean of distances) of their cluster.\n",
    "\n",
    "5) Iterate (#3 and #4).  If no clusters changed (i.e. no new points assigned to them) you have converged.  Exit.  Exit now!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Compare your cluster results with [scikit-learn Kmeans](http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html#k-means-clustering).  Since K-means is a stochastic algorithm (random initialization) your result will be slightly (but hopefully not too) different.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is tough to pick an ideal k in advance.  We can force k if we want a predetermined number of sections/topics.  But it is most likely better to vary k and let the algorithm tell us what it wants.  We can choose an optimal k using the [elbow method](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set).\n",
    "\n",
    "7) Run the algorithm with varying number of clusters $k = 1, \\dots, k_{\\mathrm{max}}$.  For each k, compute the average within-cluster dispersion $W_k$ (Lecture 6.2). Plot this for each value of k and try to find an elbow. [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set).  Is there an optimal # of K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Implement your own function to find the optimal k using the [gap statistic](https://web.stanford.edu/~hastie/Papers/gap.pdf).\n",
    "\n",
    "* Cluster the observed data, varying the number of clusters from $k = 1, \\dots, k_{\\mathrm{max}}$, and compute the corresponding $W_k$ (same as #6)\n",
    "* Generate B reference data sets (Uniform, with the same data range) and cluster each of them with varying number of clusters $k = 1, \\dots, k_{\\mathrm{max}}$. Compute the estimated gap statistic $\\mathrm{Gap}(k) = (1/B) \\sum_{b=1}^B \\log W^*_{kb} - \\log W_k$\n",
    "* With $\\bar{w} = (1/B) \\sum_b \\log W^*_{kb}$, compute the standard deviation $\\mathrm{sd}(k) = [(1/B) \\sum_b (\\log W^*_{kb} - \\bar{w})^2]^{1/2}$ and define $\\displaystyle s_k = \\sqrt{1+1/B}\\,\\mathrm{sd}(k)$\n",
    "* Choose the number of clusters as the smallest k such that $\\mathrm{Gap}(k) \\geq \\mathrm{Gap}(k+1) - s_{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Another metric to assess how well your data has been clustered is the Silhouette coefficient.  Using `scikit-learn's` metric package compute the [silhouette coefficient](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) of the clusters produced by your own K-means implementation on the iris data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit\n",
    "\n",
    "10) Implement your own function to calculate the [silhouette coefficient](https://en.wikipedia.org/wiki/Silhouette_(clustering)). ([original paper](http://www.sciencedirect.com/science/article/pii/0377042787901257))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Visualize the centroid assignments.  Create a plot of the cluster assignments on the iris data.  Each data point should be colored according to its assignment.  First make a 2-d plot of each pair of features for the iris dataset.  If you are feeling fancy make a 3-d plot.\n",
    "\n",
    " ![](images/iris_cluster.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
