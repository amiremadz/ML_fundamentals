{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 6.1 Lecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Clustering\n",
    "\n",
    "### By the End of This Lecture You Will:\n",
    "1. Be familiar with what clustering is \n",
    "2. Be able to write the algorithm of kMeans clustering\n",
    "3. Be able to describe the algorithm of Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: What is it?\n",
    "\n",
    "We have already discussed clustering in a casual context. All of the algorithms except one that you have learned so far are **supervised algorithms** in that all training data is *labeled*. \n",
    "\n",
    "This means that the label is the supervisor! There is a way of dealing with data that is **not** labeled beforehand. This is clustering. It is the only method of **unsupervised learning**, and the moniker of \"clustering\" encompasses a vast array of complex algorithms designed to extract signal from data without knowing what the signal looks like beforehand.\n",
    "\n",
    "![clustering-example](./images/clustering_example_3.png)\n",
    "\n",
    "\n",
    "This has already come up - I.E. what happens if we need to build a model and there are no labels? \n",
    "\n",
    "This can happen if:\n",
    "1. No labels have been provided and you need to set up an initial classifier based on the presumption of finite classes.\n",
    "2. You are looking to determine if continuous data can be grouped into finite classes.\n",
    "\n",
    "In both of these cases, you need to cluster the data. \n",
    "\n",
    "To be precise: Clustering is to divide data into groups (label) wherein all observations within each group meet a certain standard of similarity. \n",
    "\n",
    "\n",
    "### QUIZ: What are the two main challenges inherent to clustering?\n",
    "\n",
    "####ANSWER\n",
    "1. Process by which data is divided up\n",
    "2. Definition of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "![kmeans-example](./images/kmeans_example_2.png)\n",
    "\n",
    "Hypothesis: We can determine what the clusters are by seeking to minimize within-cluster variation. This is obtained by the definition of the centroid belonging to each cluster. The user must define the k expected clusters beforehand.\n",
    "Cost: We define the Within-Cluster-Variation: $WCV(C_k) = \\dfrac{1}{C_k}\\sum_{i, j \\in C_k}d(x_{i}-x_{j})$, where $d(x_{i}-x_{j})$ is a distance metric of your choice (usually euclidean).\n",
    "Optimization: Random search for best possible exemplar point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means algorithm\n",
    "\n",
    "    Asssign a characteristic number from 1 to K to each of N data points randomly\n",
    "    While cluster assignments keep changing:\n",
    "        For each of K clusters:\n",
    "            Calculate cluster centroid\n",
    "            For each of N points:\n",
    "                determine point distance to centroid\n",
    "        For each of N points:\n",
    "            Assign point to centroid it is closest to\n",
    "\n",
    "###QUIZ:\n",
    "What is guaranteed to happen with this algorithm? What ways are there of overcoming this problem?\n",
    "\n",
    "![kmeans-initialization](./images/kmeans_initialization.png)\n",
    "\n",
    "\n",
    "#### ANSWER: \n",
    "The end result is totally determined by the first random initialization. It can be overcome by bagging or multiple initializations and picking lowest WCV. \n",
    "\n",
    "Obviously the number K is of immense importance at several levels. Choosing K is one of the greatest challenges in clustering and will be the topic of the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Suppose we have clusters of both varying density and similar means. How can we address this problem?\n",
    "\n",
    "![hierarchical-clustering](./images/hierarchical_clustering.png)\n",
    "\n",
    "Hierarchical clustering is intended to overcome this problem by enabling the scope of the clustering to vary continuously. This is done by constructing a hierarchy tree. Hierarchical clustering is more of a concept rather than a single algorithm; we discuss a basic version of the algorithm here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering Algorithm\n",
    "\n",
    "    Initialize all points to be individual clusters\n",
    "    While n_clusters != 1:\n",
    "        For all clusters\n",
    "            Merge each cluster to its next closest cluster. This forms a new cluster.\n",
    "        Count n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting a Hierarchical Clustering\n",
    "\n",
    "Since any set of relationships captured within the tree are technically valid, the question of which of these are to be reported sets the investigator quite a task. We typically report a tree with a **cut** at a given height. Note that the height gives some sense of separation between clusters, along with their geometry. Choosing this height is, however, is a matter of great interest (just as k is above) and will be discussed as above in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of Distance\n",
    "\n",
    "The matter of distance between clusters is also one under great research and actually varies in value from dataset to dataset. Distance is typically called **linkage** in the context of hierarchical clusterings. You still need to use a metric for determining distance between points, however, we also need a way for determining distance between clusters.\n",
    "\n",
    "![cluster-distance](./images/cluster_distance.png)\n",
    "\n",
    "In practice it is common to attempt hierarchical clusterings with several different distance metrics (implying some a priori understanding of the data).\n",
    "\n",
    "![example-trees](./images/example_trees.png)\n",
    "\n",
    "### Average linkage:\n",
    "1) insensitive to outliers\n",
    "2) good comprimise between single and complete linkage\n",
    "\n",
    "### Complete linkage:\n",
    "1) less sensitive to outliers\n",
    "2) sometimes wind up with branches that overlap each other (sensitive to odd distributions)\n",
    "\n",
    "### Single linkage:\n",
    "1) More sensitive to outliers\n",
    "2) Less sensitive to odd distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
