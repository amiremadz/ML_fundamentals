{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 Boosting Practicum\n",
    "\n",
    "\n",
    "Save all work for the below section as an ipython notebook in a single directory. \n",
    "\n",
    "##Introduction to Boosting Regressors\n",
    "\n",
    "Here we will use boosting to solve a regression problem. Specifically we would\n",
    "like to predict Boston house prices based on 13 features.\n",
    "\n",
    "\n",
    "A) Import the following libraries.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.tree import DecisionTreeRegressor\n",
    "   from sklearn.ensemble import RandomForestRegressor\n",
    "   from sklearn.ensemble import GradientBoostingRegressor\n",
    "   from sklearn.ensemble import AdaBoostRegressor\n",
    "   from sklearn.datasets import load_boston\n",
    "   from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "   from sklearn.grid_search import GridSearchCV\n",
    "   from sklearn.metrics import mean_squared_error, r2_score\n",
    "   from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "   import matplotlib.pyplot as plt\n",
    "   import numpy as np\n",
    "   ```\n",
    "\n",
    "B) Import the data.\n",
    "\n",
    "   ```python\n",
    "   boston = load_boston()\n",
    "   # House Prices\n",
    "   y = boston.target\n",
    "   # The other 13 features\n",
    "   x = boston.data\n",
    "   ```\n",
    "\n",
    "   Do a `train_test_split` where `train : test` is `80 : 20`. Set\n",
    "   `random_state=1` so the exact same split can be replicated later.\n",
    "   All subsequent model selection will be carried out with the train set.\n",
    "\n",
    "C) We are interested in comparing 3 classes of ML algorithms here:\n",
    "   - `RandomForestRegressor`\n",
    "   - `AdaBoostRegressor`\n",
    "   - `GradientBoostingRegressor`\n",
    "\n",
    "   Boosted decision trees (`AdaBoost`, `GradientBoosting` & others)\n",
    "   have been shown empirically to outperform `RandomForest` on average\n",
    "   ([_Table 4 - R. Caruana et. al._](/readings/compare_ml_algo.pdf)) in terms\n",
    "   of predictive power. The runtime for boosting algorithms, as you will\n",
    "   experience, is also competitive with random forest.\n",
    "\n",
    "   As a starting point, below are instantiations of the 3 classes of\n",
    "   algorithms. I have given you a set of hyperparameters for each\n",
    "   class. Do not worry about tuning the parameters for now, we will do a\n",
    "   `GridSearch` at the end of the exercise.\n",
    "\n",
    "   ```python\n",
    "   rf = RandomForestRegressor(n_estimators=100,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=1)\n",
    "\n",
    "   gdbr = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                     loss='ls',\n",
    "                                     n_estimators=100,\n",
    "                                     random_state=1)\n",
    "\n",
    "   abr = AdaBoostRegressor(DecisionTreeRegressor(),\n",
    "                            learning_rate=0.1,\n",
    "                            loss='linear',\n",
    "                            n_estimators=100,\n",
    "                            random_state=1)\n",
    "   ```\n",
    "   **Note:**\n",
    "   `n_jobs=-1` _allows the process to be run on multiple cores on\n",
    "   your computer. Only parallel algorithms such as_ `RandomForest` _can\n",
    "   do that. Boosting is sequential (the current step depends on the residuals\n",
    "   from the previous) and does not have that option._\n",
    "   `n_jobs=-1` _is not a hyperparameter._\n",
    "\n",
    "\n",
    "4. Using `cross_val_score` in `sklearn`, define a function that \n",
    "   calculates the cross-validated train MSE and R2 for `AdaBoostRegressor`,\n",
    "   `GradientBoostingRegressor`, `RandomForestRegressor`.\n",
    "\n",
    "   **Your output should be similar to this (Do not worry if the numbers do\n",
    "   not match up exactly):**\n",
    "\n",
    "   ```\n",
    "   RandomForestRegressor Train CV | MSE: 9.881 | R2: 0.866\n",
    "   GradientBoostingRegressor Train CV | MSE: 8.453 | R2: 0.886\n",
    "   AdaBoostRegressor Train CV | MSE: 10.335 | R2: 0.861\n",
    "   ```\n",
    "\n",
    "   Which of the models cross validates the best? Why is it inappropriate\n",
    "   to make a judgement on the performance of the models\n",
    "   based only on the evidence we have thus far?\n",
    "\n",
    "5. Define a new instance of `GradientBoostingRegressor` with the exact same\n",
    "   hyperparameters as above, except change the `learning_rate` to `1`\n",
    "   (instead of `0.1`). Calculate the cross-validated train MSE.\n",
    "    What do you notice?\n",
    "\n",
    "6. We're going to make a plot to help us understand the impact of the learning rate\n",
    "   and the improvements in error after each iteration of the boosting.\n",
    "   \n",
    "   [`staged_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict) is a method in both\n",
    "   `GradientBoostingRegressor` and `AdaBoostRegressor`. It allows us to get\n",
    "   predictions from the estimator after each iteration of the boosting.\n",
    "   \n",
    "   Using\n",
    "   `stage_predict`, define a function `stage_score_plot` that calculates the test and train\n",
    "   MSE from each estimator. Do the calculations for `GradientBoostingRegressor`\n",
    "   with `learning_rate=1` and `learning_rate=0.1`\n",
    "\n",
    "   ```python\n",
    "   def stage_score_plot(model, train_x, train_y, test_x, test_y):\n",
    "      '''\n",
    "      INPUT:\n",
    "         model: GradientBoostingRegressor or AdaBoostRegressor\n",
    "         train_x: 2d numpy array\n",
    "         train_y: 1d numpy array\n",
    "         test_x: 2d numpy array\n",
    "         test_y: 1d numpy array\n",
    "      \n",
    "      Create a plot of the number of iterations vs the MSE for the model for\n",
    "      both the training set and test set.\n",
    "      '''\n",
    "      \n",
    "      #### YOUR CODE HERE ####\n",
    "   ```\n",
    "   \n",
    "   You should be able to run your code like this:\n",
    "   \n",
    "   ```python\n",
    "   stage_score_plot(gdbr, train_x, train_y, test_x, test_y)\n",
    "   plt.legend()\n",
    "   plt.show()\n",
    "   ```\n",
    "   \n",
    "   And get a result which looks like this:\n",
    "\n",
    "   ![stage_score_plot](images/stage_score_plot.png)\n",
    "   \n",
    "   In order to get the labels for the plot, you can use `model.__class__.__name__` to get the model name and `model.learning_rate` to get the learning rate.\n",
    "\n",
    "7. Use your `stage_score_plot` function to make a plot that shows the error\n",
    "   for gradient boosting with a learning rate of 0.1 and 1.\n",
    "\n",
    "   Since you are comparing two models and we're showing both the training and\n",
    "   test error, you should have 4 lines on your graph.\n",
    "\n",
    "8. Given your plot, explain the behavior of the test / train curves\n",
    "   for the two (0.1 and 1) learning rates. With a lower learning rate (0.1),\n",
    "   what is necessary to obtain a low test error?\n",
    "\n",
    "9. Using the `stage_score_plot` function, make a plot like you did above of\n",
    "    the MSE for `GradientBoostingRegressor` with `learning_rate=0.1`.\n",
    "\n",
    "    Add a horizontal line to indicate where the `RandomForestRegressor` test\n",
    "   error is at.\n",
    "\n",
    "   Your plot should look something like this:\n",
    "\n",
    "   ![gradient boosting](images/gradboost.png)\n",
    "\n",
    "   a. How many iterations does it take until Gradient Boosting beats Random Forest?\n",
    "\n",
    "10. Make a similar plot for `AdaBoost`. Again have the `learning_rate=0.1`\n",
    "    and add a horizontal line for the Random Forest test error.\n",
    "\n",
    "    Don't expect AdaBoost to be as smooth as the Gradient Boosting graph.\n",
    "\n",
    "    a. How many iterations does it take till AdaBoost beats Random Forest?\n",
    "\n",
    "11. As seen above when we compared two learning rates, suboptimal hyperparameters\n",
    "    can give rise to higher error\n",
    "    (MSE). Therefore, we aim to search for the set of hyperparameters that\n",
    "    would give us the lowest cross-validated train error. The search of these\n",
    "    hyperparameters is known as grid-search. For each hyperparameter, a set\n",
    "    of values are specified. The combination of the hyperparameters at different\n",
    "    values will constitute the search space. We try each possible combination\n",
    "    of parameters and find the combination which minimizes error.\n",
    "\n",
    "    Use `GridSearchCV` for to find the best `RandomForestRegressor`\n",
    "    and `GradientBoostRegressor` models respectively.\n",
    "    Remember to specify `n_jobs=-1` in `GridSearchCV` to use all the cores of your\n",
    "    machine and speed up your search.\n",
    "\n",
    "    Here are some values to start out with trying for hyperparameters for Random Forest:\n",
    "\n",
    "    ```python\n",
    "    random_forest_grid = {'max_depth': [3, None],\n",
    "                          'max_features': ['sqrt', 'log2', None],\n",
    "                          'min_samples_split': [1, 2, 4],\n",
    "                          'min_samples_leaf': [1, 2, 4],\n",
    "                          'bootstrap': [True, False],\n",
    "                          'n_estimators': [10, 20, 40],\n",
    "                          'random_state': [1]}\n",
    "\n",
    "    rf_gridsearch = GridSearchCV(RandomForestRegressor(),\n",
    "                                 random_forest_grid,\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=True,\n",
    "                                 scoring='mean_squared_error')\n",
    "    rf_gridsearch.fit(train_x, train_y)\n",
    "\n",
    "    print \"best parameters:\", rf_gridsearch.best_params_\n",
    "\n",
    "    best_rf_model = rf_gridsearch.best_estimator_\n",
    "    ```\n",
    "\n",
    "    Feel free to change it to try a different set of parameters.\n",
    "\n",
    "    Note that this will take about 3-5 minutes to run. The total number of combinations is:\n",
    "    `2 * 3 * 3 * 3 * 2 * 3 * 1 = 324`. We are trying each of these possibilities!\n",
    "\n",
    "    a. What are the optimal parameters?\n",
    "\n",
    "    b. What is the MSE you get on the test set with these parameters?\n",
    "\n",
    "    c. How does this compare with the MSE with the default parameters?\n",
    "\n",
    "12. Go through the same process for `GradientBoosting`. Try several values for\n",
    "    these hyperparameters:\n",
    "\n",
    "    * `learning_rate`\n",
    "    * `max_depth`\n",
    "    * `min_samples_leaf`\n",
    "    * `max_features`\n",
    "    * `n_estimators`\n",
    "    \n",
    "    If you're unsure what values to include, take a look at sklearn's default. Include the default value and at least \n",
    "    one value less than and greater than the default value. Here's the [docs on Gradient Boosting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) where you can see the defaults.\n",
    "    If you want a set of starting parameters, [here](https://gist.github.com/pprett/3989337#file-grid_search-py-L115) is\n",
    "    a reference.\n",
    "\n",
    "    a. What are the parameters that give the optimal model?\n",
    "\n",
    "    b. How does the MSE for this model compare with the original MSE you got with Gradient Boosting\n",
    "    before tuning the parameters?\n",
    "\n",
    "13. Go through the same process for `AdaBoost`. Note you need to define different hyperparameters\n",
    "    in the base estimator to perform the gridsearch. This will also take much longer to run,\n",
    "    so feel free to just leave it running after you figure out some parameters to try.\n",
    "\n",
    "## Studying the AdaBoost Classifier\n",
    "We have encountered `AdaBoostRegressor` and its gradient\n",
    "descent variant, `GradientBoostingRegressor`. The base form of AdaBoost was\n",
    "introduced in 1995 as an ensemble classifier, `AdaBoostClassifier`.\n",
    "Understanding `AdaBoostClassifier` is regarded as the defacto\n",
    "introduction to the world of seemingly endless variants of boosting algorithms\n",
    "([refs](readings)). To gain a more entrenched understanding of boosting\n",
    "in general, I would recommend [this](readings/explaining_boosting.pdf).\n",
    "\n",
    "Make yourself familiar with the reading if you are not comfortable.\n",
    "\n",
    "\n",
    "## Implementing the AdaBoost Classifier\n",
    "Here we will build a simplified version of `AdaBoostClassifier`. In this case,\n",
    "our classifier, `AdaBoostBinaryClassifier`, will only predict binary outcomes.\n",
    "The starter code is in the [code](code) folder. The `.py` file contains the \n",
    "core functions you would have to implement this afternoon. Fill in the rest depending \n",
    "on your progress.\n",
    "\n",
    "Your code should exactly implement this pseudocode:\n",
    "\n",
    "![adaboost](images/adaboost_algorithm_1.png)\n",
    "\n",
    "We're going to be using the spam dataset. It's in the [data](data) folder. You can see the feature names [here](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names).\n",
    "\n",
    "Here's how you should be able to run your code after you're finished:\n",
    "\n",
    "```python\n",
    "from boosting import AdaBoostBinaryClassifier\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data = np.genfromtxt('data/spam.csv', delimiter=',')\n",
    "\n",
    "y = data[:, -1]\n",
    "x = data[:, 0:-1]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y)\n",
    "\n",
    "my_ada = AdaBoostBinaryClassifier(n_estimators=50)\n",
    "my_ada.fit(train_x, train_y)\n",
    "print \"Accuracy:\", my_ada.score(test_x, test_y)\n",
    "```\n",
    "\n",
    "1. Take a look at the `__init__` method. You shouldn't need to change anything here. Note how we are creating Decision Trees that are just stumps! (max depth is 1). \n",
    "\n",
    "2. Implement the `_boost` method. This will be doing steps (a)-(d) inside the for loop.\n",
    "\n",
    "    Because we need many copies of the estimator, the first step is to clone it. This code is given for you.\n",
    "\n",
    "    In this function `sample_weight` refers to the *wi*'s in the above description of the algorithm.\n",
    "\n",
    "    You will need to do these steps:\n",
    "\n",
    "    * Fix the Decision Tree using the weights. You can do this like this: `estimator.fit(x, y, sample_weight=sample_weight)`\n",
    "    * Calculate the error term (`estimator_error`)\n",
    "    * Calculate the alphas (`estimator_weight`)\n",
    "    * Update the weights (`sample_weight`)\n",
    "\n",
    "3. Implement the `fit` method. This is steps 1 and 2 from above.\n",
    "\n",
    "    You should have a for loop that calls your `_boost` method `n_estimators` times. Make sure to save all the estimators in `self.estimators_`. You also need to save all the estimator weights in `self.estimator_weight_`.\n",
    "\n",
    "4. Implement the `predict` method. This is step 3 from above.\n",
    "\n",
    "    Note that the algorithm considers the predictions to be either -1 or 1. So once you get predictions back from your Decision Trees, change the 0's to -1's.\n",
    "\n",
    "5. Implement the `score` method.\n",
    "\n",
    "    This should call the predict method and then calculate the accuracy.\n",
    "\n",
    "6. Load the file `data/spam_data.csv` into a dataframe. Use `train_test_split` to create test and train sets.\n",
    "   Train your implementation of `AdaBoostBinaryClassifier` on the train set and get the train and test accuracy scores. \n",
    "   Compare your results with sklearn's [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).\n",
    "   You should get approximately the same accuracy. \n",
    "   \n",
    "   **Review the steps to implement the algorithm and make sure you have understood the underpinnings of boosting.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
