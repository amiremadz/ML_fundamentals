{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing  import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following exercises, you will be using the Scikit-learn version of SVC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 1: Preprocessing data for SVM\n",
    "\n",
    "Intuitively, it makes sense that SVMs might need scaling. Since SVMs are sensitive to the \n",
    "distance of points relative to a hyperplane, if one dimension had units in the thousands, \n",
    "the distance along that dimension would overwhelm another dimension with values in `[0,1]`. \n",
    "And the model would focus disproportionately on the larger dimension. Scaling overcomes this.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Load the file `data/non_sep.csv` into a dataframe. Note the file type. Call the features `X` and `y`. **WARNING**: This is an important part of all data science. Make sure that the labels `y` are correctly set up. They should be labeled either `1` or `0`.\n",
    "\n",
    "2. Use some EDA to look at the features and structure of the dataset.\n",
    "\n",
    "3. Scale both features using [`scale`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html).\n",
    "\n",
    "4. Plot the decision boundary along with the data points. Use the plot_decision function below.\n",
    "\n",
    "5. Fit the linear SVC again without scaling the data. What do you notice? The SVC should fit much faster when the\n",
    "   features are scaled, otherwise in this case it should take around 3 minutes to run.\n",
    "\n",
    "6. Use `cross_val_score` to compute the average `accuracy` of a 5-fold cross validation of the scaled model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python \n",
    "def plot_decision(model, X, y, fig, ax, margins=False):\n",
    "    \"\"\"INPUTS:\n",
    "        model: a trained classifier, with a predict method\n",
    "        X: The feature matrix\n",
    "        y: The response vector\n",
    "        fig: Matplotlib figure object\n",
    "        ax: Matplotlib axis object\n",
    "       OUTPUTS:\n",
    "          OBJECT: fig, ax\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    xmin = np.min(X[:, 0]) - 0.2\n",
    "    xmax = np.max(X[:, 0]) + 0.2\n",
    "    ymin = np.min(X[:, 1]) - 0.2\n",
    "    ymax = np.max(X[:, 1]) + 0.2\n",
    "\n",
    "    # find decision boundary \n",
    "    model.fit(X, y)\n",
    "    w = model.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(xmin,xmax, 1000)\n",
    "    yy = a * xx - model.intercept_[0] / w[1]\n",
    "\n",
    "    def plot_vector(ax1):\n",
    "        # find support vectors and margins\n",
    "        b = model.support_vectors_[0]\n",
    "        minus_plane = a * xx + (b[1] - a * b[0])\n",
    "        b = model.support_vectors_[-1]\n",
    "        plus_plane = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "        # plot support vectors\n",
    "        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=50, c='k')\n",
    "\n",
    "        # plot margin\n",
    "        ax.plot(xx, minus_plane, 'r--')\n",
    "        ax.plot(xx, plus_plane, 'b--')\n",
    "\n",
    "\n",
    "    if margins:\n",
    "        plot_vector(ax)\n",
    "\n",
    "    reg = (\"; C = \" + str(model.get_params()['C'])) if type(model) == SVC else ''\n",
    "    # plot decision boundary\n",
    "    colors = ['r' if z==1 else 'b' for z in y]\n",
    "    ax.plot(xx, yy, label=str(type(model)).split(\".\")[-1][:-2] + reg)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    plt.xlim((xmin,xmax))\n",
    "    plt.ylim((ymin, ymax))\n",
    "    return fig, ax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2: Hyperparameter C\n",
    "You have probably noticed that the dataset used in `Part 1` is not linearly separable.\n",
    "In reality, you will rarely encounter a dataset that is linearly separable. In such cases,\n",
    "SVMs have a tradeoff between maximizing the margin and minimizing the \n",
    "classification error. The hyperparameter `C` controls this tradeoff. It is equivalent \n",
    "to 1/λ (the regularization term from Ridge/Lasso), thus smaller values of `C` result in larger λ.\n",
    "Continue using `data/non_sep.csv` for this part.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Loop through a list of values of `C` in the list `0.001`, `0.01`, `0.1`, `1`, `10`, `100`, and train the linear `SVC` with each of the `C` values. \n",
    "\n",
    "2. Plot the respective decision boundaries: this will give you an intutition as to how changes to `C` affect the decision boundary. \n",
    "\n",
    "3. Plot how the cross-validated accuracy changes with `C`, and find the highest cross-validated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 3: Kernel Tricks\n",
    "\n",
    "SVMs can use kernel functions to transform the features into a new feature\n",
    "space. Thus far we have been using a linear kernel, but SVMs can use non-linear\n",
    "kernels.\n",
    "\n",
    "![](http://rvlasveld.github.io/images/oc-svm/visualization.gif)\n",
    "\n",
    "Two of the most common nonlinear kernels are the polynomial kernel function and\n",
    "the Gaussian kernel function (also known as the Radial Basis Function, or RBF).\n",
    "\n",
    "Both of these kernels transform the data into a new space where the\n",
    "data may be (more) linearly separable.\n",
    "\n",
    "####RBF Kernel\n",
    "\n",
    "`K(x, z) = exp(gamma * (distance(x, z))^2)`\n",
    "\n",
    "`gamma` is a hyperparameter that determines the spread of the Gaussian around each point. `distance` is Euclidean distance.\n",
    "\n",
    "[Wikipedia - RBF\n",
    "Kernel](http://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n",
    "\n",
    "####Polynomial Kernel\n",
    "\n",
    "`K(x, z) = (1 + x.T.dot(z))^d`\n",
    "\n",
    "\n",
    "`d` is a hyperparameter that determines the degree of the polynomial transform.\n",
    "\n",
    "[Wikipedia - Polynomial Kernel](http://en.wikipedia.org/wiki/Polynomial_kernel)\n",
    "\n",
    "In practice, RBF kernels are more often used (scikit learn uses the RBF as the\n",
    "default kernel in SVC). The polynomial kernel at high degrees often leads to\n",
    "overfitting.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Use your skills in parsing and loading data to train a model on `non_linear.csv` using  the RBF kernel. Use the ravel function as above. For now, use the parameters `C = 1` and `gamma = 1`.\n",
    "    \n",
    "2. Use the function below to visualize non-linear decision boundaries of the models.\n",
    "\n",
    "```python\n",
    "def decision_boundary(clf, X, Y, h=.02):\n",
    "    \"\"\"INPUTS:\n",
    "        clf: a trained classifier, with a predict method\n",
    "        X: The feature matrix\n",
    "        y: The response vector\n",
    "        h: meshgrid fineness\n",
    "       OUTPUTS:\n",
    "          None\n",
    "    \"\"\"\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 4: Grid Search\n",
    "\n",
    "Let's say we have a model using a polynomial kernel but we have yet to tune our\n",
    "hyperparameters. In this case, there are two: first, `C`, as always, and second,\n",
    "`degree`, the degree of the polynomial kernel. What we'd like to know is which\n",
    "combination of hyperparameters yields the best results. Scikit-learn's\n",
    "[`grid_search`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html)\n",
    "module has just what we need.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. In your instantiation `gsCV` of `GridSearchCV`, set `scoring='accuracy'` to set the scoring\n",
    "   type to classification accuracy and `cv=10` to set the number of folds in K-fold CV to 10.\n",
    "   \n",
    "2. You can find the result of the grid search with `gsCV.grid_scores_` and the winner in `gsCV.best_params_`. Conduct a grid search over a range of possible values of `C` and `gamma` and find the values of `C` and `gamma` that maximize the cross-validation accuracy.\n",
    "\n",
    "3. Compare the accuracy of the tuned RBF model to the untuned model.\n",
    "\n",
    "4. Graph the decision boundary of the tuned model and compare it with the decision boundary of the untuned model.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 5: Real-world Modeling\n",
    "\n",
    "In most cases, we won't be able to visualize our SVM solution; we'll have too many variables. In such cases, we have to rely exclusively on cross-validation to select from among competing solutions.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Load the file `data/dataset_1.csv` into a dataframe.\n",
    "\n",
    "2. Scale the features.\n",
    "\n",
    "3. Try different models, kernels etc. and see which performs best in cross validation. Perform `GridSearchCV`\n",
    "   as appropriate as it is a timely process. You should be able to achieve over **95% accuracy** with this dataset.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 6: SVM Applications to Wide and Tall Data\n",
    "\n",
    "In general, as the number of features increases, the gain of accuracy of using an RBF kernel instead of a linear\n",
    "kernel becomes smaller. Here we explore a \"wide\" dataset where 7129 features of a patient is used used predict if he/she\n",
    "has leukemia.\n",
    "\n",
    "What are the properties of a wide dataset? What does it mean with respect to the linear algebra of the data? What would you expect for a \"typical\" model applied to such data?\n",
    "\n",
    "1. Load the file `data/dataset_2.csv` into a dataframe\n",
    "\n",
    "2. Fit and perform gridsearch with a RBF kernel and a linear kernel.\n",
    "\n",
    "3. Compare the performance of the two kernels. What are the advantages of using a linear kernel when the performance\n",
    "   is similar to an RBF?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here we explore a \"tall\" dataset, where there are 200,000 entries and 13 rows. What are the properties of a tall dataset? What does this condition mean with respect to linear algebra?\n",
    " \n",
    "1. Load the file `data/dataset_3.csv` into a dataframe. It has been scaled, so you will not need to scale it.\n",
    "   Subsample 3,000 rows to shorten our training time. Normally if time is not a constraint, then subsample is \n",
    "   not necessary.\n",
    "   \n",
    "2. Gridsearch to select the best model for linear and RBF kernels.\n",
    "   Comment on their relative performance. Be careful not to search the parameter space too\n",
    "   exhaustivley, or you'll find it takes a long time. Start with no more than 5 values.\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 7: Multiclass SVM\n",
    "\n",
    "So far, we've focused on two-class classification problems, but what if we have multiple classes?\n",
    "We might want to classify which section of a newspaper an article belons in, what digit is \n",
    "represented by an image, etc. So how can we extend our binary classifiers to deal with such cases?\n",
    "\n",
    "To train a binary classifier to identify multiple classes often something called one-vs-rest is used, which trains each possible class against all other classes at a time. Another option, implemented by default in SVC is one-vs-one, which trains each class against the other.\n",
    "\n",
    "   1. How many models need to be trained to predict all classes in a one-vs-rest approach?\n",
    "   2. In a one-vs-one? (Remember, you don't need to train both x vs. y and y vs. x).\n",
    "\n",
    "In scikit-learn the default SVC handles multiclass internally  one-vs-one , but other \n",
    "classification algorithms use one-vs-rest.  For this exercise we will be \n",
    "using [scikit-learns](http://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest) \n",
    "one-vs-the-rest classifier.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Using the [digit data set](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)\n",
    "   create a one-vs-one classifier and a one-vs-rest classifier and train them with LinearSVC \n",
    "   and a LogisticRegression. __For the OvR logistic regression, you'll have to use the builtin multi-class support instead of the OneVsRest class. Use the one-vs-rest classifier for both the \n",
    "   LinearSVC and the LogisticRegression. Compare the performance of each for precision, recall, \n",
    "   and accuracy. Feel free to sample the dataset if it is taking too long to run\n",
    "   \n",
    "<br>   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
