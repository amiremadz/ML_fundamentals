{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills Test 1\n",
    "\n",
    "Solve problems that you are comfortable with first, to the best of your ability. Realize that many, if not most, students may not complete the entire test, and thus you need to focus on *doing your best on what you can*. \n",
    "\n",
    "* Time: 90 minutes\n",
    "* Closed Book\n",
    "* Individual\n",
    "\n",
    "Remember: Everything is Showbiz. Break a leg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Examination\n",
    "\n",
    "Focus on producing concise, complete answers to the written sections. Place answers to each question in the appropriate cell. Written answers that are complete but spitballing (i.e. B.S.) will be unlikely to recieve any points. Partial answers with a few correct elements are much more likely to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussing Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write a concise description of each of the elements for k-Nearest Neighbors:\n",
    "\n",
    "* Hypothesis\n",
    "* Cost\n",
    "* Optimization\n",
    "\n",
    "(It is true that these elements do not map cleanly to those of other algorithms you have learned, however you should be able to take a moment and reason out approximately what they should be.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hypothesis : K nearest neighbour hypothesis states that a point can be classified by the neighbour it is nearest to.\n",
    "    eg: If we take a sample of people belonging to different country we need to classify them according to the country of their origin.\n",
    "        \n",
    "\n",
    "        \n",
    "Cost : Cost of K - nearest neighbour is the selction of the class label. We need to find the  class of the data point.\n",
    "    \n",
    "Optimization : We need to optimize the eucilidean distance between the two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) k-Nearest Neighbors: what are the:\n",
    "\n",
    "* Use cases\n",
    "* Strengths\n",
    "* Weaknesses \n",
    "\n",
    "(short answers!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Use cases : Suppose we need to classify movies based on different genres. Like action,romantic etc.\n",
    "    We group the data or form clusters where the data point exhibits the similar behavior. So we have cluster of romantic movie,action movies etc.\n",
    "    \n",
    "Strengths : Eventhough we are not doing prediction on class label we are grouping or clustering the data of similar\n",
    "    behaviors. This helps in identifying to which group a datapoint belongs to.\n",
    "\n",
    "Weakness : Some times it can be difficult to separate out the clusters from the sample data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) You have produced the below diagram of performance for your k-NN algorithm. Assume that you have used euclidean distance for your distance metric. Explain in a concise discription what is happening and the *mathematical reason as to why* it is happening. \n",
    "\n",
    "\n",
    "\n",
    "![dvp](./images/dvp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Answer3\n",
    "\n",
    "THis is what is called as curse of dimensionality. As the dimensionality increases then in k-mean it becomes \n",
    "difficult to separate out or distinguish  the different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a concise description (including mathematical formulae) of each of the elements for Regularized Logistic Regression:\n",
    "\n",
    "* Hypothesis\n",
    "* Cost\n",
    "* Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis :\n",
    "\n",
    "$h=\\frac{1}{1+e^{-t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost : \n",
    "$$ \\  cost regularised L2 = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} \\beta_j^2$$\n",
    "\n",
    "This is Ridge or L2 regularisation.\n",
    "\n",
    "Cost : \n",
    "$$ \\  cost regularised L1 = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} \\beta_j$$\n",
    "\n",
    "THis is LASSO or L1 regularisation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Optimization is done through Gradient Descent method of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Logistic Regression: Why would we choose to use a logistic regression over any other type of regression? What is the real difference between a logistic regression and any other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In Logistic regression  we make discrete predictions. So in a case where we need boolean classification it can be useful.\n",
    "the value of sigmoid function lies between 0 and 1. We label the class in 0 and 1.\n",
    "\n",
    "Difference between the othe regression is we find a line of best fit and minimise the standard residual error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) How do we measure the performance of Logistic Regression? How does this differ from measurement of the performance of standard Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We can measure the performance of logistic regression using confusion matrix. Since it label of logistic regression is classified into 0 and 1\n",
    "confusion matrix canbe used to evaluate the performance , i.e accuracy, precision , recall .\n",
    "\n",
    "In standard regression we find the line of best fit and we are trying to minimise the error residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Write a concise description (including mathematical formulae) of each of the elements for Naive Bayes:\n",
    "\n",
    "* Hypothesis\n",
    "* Cost\n",
    "* Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hypothesis : The labels or the contributing  members of the class c should be independent.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cost function is the log likelihood in naive bayes.\n",
    "\n",
    "Cost : J(C/X)=log (p(C))+log(p(x1/c))+log(p(x2/c))+........log(p(xn/c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Optimization: It is done by the training data set and can be done only once. \n",
    "    Although there are suggestions to add priors so that model is adapted but its not recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Why is Naive Bayes so naive? Use the mathematics to provide an explanation. The best answer will discuss the probability theory involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Naive bayes states that P(A/B)= p(B/A). p(A)/p(B) \n",
    "\n",
    "in this formula it states that given a condition B what is the probability of the occurence of A.\n",
    "\n",
    "So in this we make assumption that \n",
    "the value of a particular feature is independent of the value of any other feature, given the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Why would I prefer to use lasso regression instead of ridge regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lasso regression is preferred to ridge regression in cases where:\n",
    "    \n",
    "a)Large sparse data sets. while in ridge we have non sparse data.\n",
    "b)It turns most of the regressors to zero while in ridge it suppresses leading regressors lightly and lagging regressors heavily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics\n",
    "\n",
    "These questions will require a little extra writing than the above section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why does the Bias-Variance tradeoff occur? More specifically, what causes it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bias Variance trade off occur because if our model has a high bias to improve it \n",
    "we need to add more features , we need to increase complexity of the model and decrease regularisation.\n",
    "\n",
    "IN case if model has high variance in our model we need to make it better by\n",
    "decreasing the number of features, reduce the complexity of model , we need to use more trainingdata set\n",
    "and increase regularisation.\n",
    "\n",
    "So if we refer to the chart between bias, variance , complexity of model , error  we need to select a model \n",
    "where we can get a optimised value of error, bias, variance.\n",
    "So at a specific optimised point we will get a model which will be the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) How do we know that a model is underfit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model is underfit when there is high bias.Error will be consistently high in this type of model.\n",
    "There will be low variance and with new data we will get approximately same type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How do we know that a model is overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model is overfit when it represents high variance.\n",
    "One can make the model complex(like increasing the polynomial degree) to fit it and \n",
    "the predictions will be very good on that data set but for a new data set the model will not be good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) How do bias and variance relate to precision and accuracy? How do they relate to Type I and Type II errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bias can occur when sampling of data is biased so in such case our accuracy may be high but it is not a good model.\n",
    "Bias model can be accurate and precise. Precise is TP/TP+FP= TP / Predicted yes so there will be type 1 error.\n",
    "\n",
    "Variance model can be precise but not accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Why is $R^{2}$ not a good metric to compare two regression models? What would be a better choice to compare them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is not a good metric to compare 2 regression models as it is not able to estimate the proper variance.\n",
    "Adjusted $R^2$ is the better matrix to comapre two regression models or we can also check the AIC , BIC values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 6-8\n",
    "\n",
    "Below is a ROC curve detailing the confusion matrix of a binary ultrasound test for uterine cancer. The ultrasound device makes it possible to measure its thickness. The idea of the test is to measure the thickness of the wall and set a threshold of thickness that is \"normal\". \n",
    "\n",
    "When the test measures a thickness greater this number, the patient will be labeled as a cancer risk and the test will be \"positive\". Below the number the test will be \"negative.\"\n",
    "\n",
    "Of course there are limitations. \n",
    "\n",
    "![uterine_roc](./images/ROC_endometrial.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Write a *brief* description of what is happening in the above figure, discussing the role of sensitivity and specificity with respect to the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Initially in the graph true positive rate is increasing while the false positive rate is increasing gradually.\n",
    "After a certain point of threshold the graph converges and we have almost same True positive rate.\n",
    "Yanden index =J= sensitivity + specificity -1 describes the performance of the classifier. So if we have this\n",
    "value closer to 1 its a good classifier . Sensitivity is TP/TP+FN and Specificity is TN /TN+FP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Suppose that this test is expensive and lacks predictive value of outcome and thus we want to minimize the number of false positives while still keeping the test usefully diagnostic. Where should we set the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We should set the threshold near to 10 mm ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Suppose that this test is cheap and it doesn't matter to us what the human implications are of giving millions of women a year the belief that they are soon to be diagnosed with uterine cancer. Our intent is to diagnose every possible case while keeping the number of false positives reasonable. Where should we set the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "we should set the threshold at 4mm because after that it looks from the graph thaat FP rate is increasing at fast rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Examination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bayes\n",
    "\n",
    "You will need to use the original lab for Naive Bayes.\n",
    "\n",
    "Naive Bayes has numerous variants. The only difference between each of these variants and Multinomial Bayes is in the calculation of the distribution of likelihoods.  For Bernoulli Bayes we have the following likelihood function:\n",
    "\n",
    "$$p(y|x) = p(x_{i}|y)^{N_{1}}(1-p(x_{i}|y))^{N_{0}}$$\n",
    "\n",
    "Where $y$ is the class to be predicted, $p(x_{i}|y)$ is the frequency of a feature (word) given the class. However, we account for the prediction of the class discussing the frequencies of appearance each label has. That is to say, $N_{1}$ is the number of counts of a given feature (word) within that class and/or prediction. $N_{0}$ is the number of counts when the given feature does *not* appear within a given class! Hence we are predicting *negative* relationships within a class as well as positive relationships.\n",
    "\n",
    "you are permitted to study this [resource](http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html) and compare with [this one](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html). Don't waste too much time reading!!\n",
    "\n",
    "\n",
    "1) Write the log-likelihood cost function needed for BernoulliBayes.\n",
    "\n",
    "2) Inherit from the `NaiveBayes` class from the lab to create a `BernoulliBayes` subclass.\n",
    "\n",
    "3) Overwrite the `_compute_likelihood()` function of the `NaiveBayes` parent class (within the subclass) to reflect the log-likelihood of the Bernoulli Bayes variant.\n",
    "\n",
    "4) Run your `BernoulliBayes` model on the same test harness used in the lab.  How does it score compared to the Multinomial Naive Bayes? Why would this behavior occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loglikelihood_Bernoulli():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 & 3\n",
    "#Passing the object of Niave byes to the bernoulli bayes class and then we can inherit its feature and override it.\n",
    "\n",
    "def class BernoulliBayes(NaiveBayes):\n",
    "        def compute_likelihood(self,X,y):\n",
    "            #We will write our own implementation or override for the likelihood here and return it \n",
    "            return self.likelihood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
