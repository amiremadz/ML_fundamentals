{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills Test 1\n",
    "\n",
    "Solve problems that you are comfortable with first, to the best of your ability. Realize that many, if not most, students may not complete the entire test, and thus you need to focus on *doing your best on what you can*. \n",
    "\n",
    "* Time: 90 minutes\n",
    "* Closed Book\n",
    "* Individual\n",
    "\n",
    "Remember: Everything is Showbiz. Break a leg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Examination\n",
    "\n",
    "Focus on producing concise, complete answers to the written sections. Place answers to each question in the appropriate cell. Written answers that are complete but spitballing (i.e. B.S.) will be unlikely to recieve any points. Partial answers with a few correct elements are much more likely to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussing Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write a concise description of each of the elements for k-Nearest Neighbors:\n",
    "\n",
    "* Hypothesis\n",
    "* Cost\n",
    "* Optimization\n",
    "\n",
    "(It is true that these elements do not map cleanly to those of other algorithms you have learned, however you should be able to take a moment and reason out approximately what they should be.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    3 point response:\n",
    "\n",
    "    * Hypothesis: Members of a test set can be classified by the spatial relationships maintained with neighborhoods of training points.\n",
    "    * Cost: a) Each class votes for a given test point according to its neighborhood proximity to that point (using a given distance norm). b) The class is assigned based on the largest number of votes. The neighborhood is defined as having k or more neighbors to the the test point (thus k is effectively a threshold).\n",
    "    * Optimization: The training set is only learned once and so the algorithm is already optimal within the definition of the problem.\n",
    "\n",
    "    Deduct 1 full point for conceptual deviations from each of these terms. 0.5 point deduction if important pieces are missing (in this case only the cost function has several parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) k-Nearest Neighbors: what are the:\n",
    "\n",
    "* Use cases\n",
    "* Strengths\n",
    "* Weaknesses \n",
    "\n",
    "(short answers!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    \n",
    "    3 point response:\n",
    "\n",
    "    * Use cases: Where the problem can be formulated in two dimensions and does not include strongly overlapping clusters of varying density.\n",
    "    * Fast, reasonably robust to small amounts of noise.\n",
    "    * There are many weaknesses, especially the curse of dimensionality and variable density.\n",
    "\n",
    "    Deduct 1 full point for conceptual deviations from each of these terms. 0.5 point deduction if important pieces are missing (in this case only the cost function has several parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) You have produced the below diagram of performance for your k-NN algorithm. Assume that you have used euclidean distance for your distance metric. Explain in a concise discription what is happening and the *mathematical reason as to why* it is happening. \n",
    "\n",
    "\n",
    "\n",
    "![dvp](./images/dvp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    3 point response:\n",
    "\n",
    "    This is an illustration of the above questions. As the number of dimensions increase, the (euclidean) distances between a point $x_i$ belonging to actual class $C1$ and training points belonging to other classes $C2 \\cdots CN$ begin to converge to similar values. Thus all classes begin to vote equally, and the model becomes unable to distinguish between similar classes. The effects are noticeable even at four and five dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a concise description (including mathematical formulae) of each of the elements for Regularized Logistic Regression:\n",
    "\n",
    "* Hypothesis\n",
    "* Cost\n",
    "* Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    \n",
    "3 point response:\n",
    "\n",
    "    * Hypothesis: The training set points have a binary relationship, and belong to one of two classes. The underlying variables that determine the binary relationship have a linear relationship amongst each other (linear regression).\n",
    "    * Cost: \n",
    "\n",
    "$$J = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} \\beta_j^2$$\n",
    "\n",
    "    * Optimization: Gradient descent. (Writing the gradient cost is welcome but not necessary)\n",
    "    \n",
    "$$-\\frac{1}{n} (\\sum_{i=1}^{n} \\left( y_i - p(x_i) \\right) x_{ij}+\\lambda \\sum_{j = 1}^{p} \\beta_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Logistic Regression: Why would we choose to use a logistic regression over any other type of regression? What is the real difference between a logistic regression and any other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    \n",
    "    3 point response:\n",
    "\n",
    "    A logistic regression is used specifically for binary classification. The real difference between a logistic regression and any other is simply the inclusion of the logistic function into the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) How do we measure the performance of Logistic Regression? How does this differ from measurement of the performance of standard Regression?\n",
    "\n",
    "    A logistic regression is typically measured in terms of its confusion matrix - particularly the true positive rate (sensitivity) and false positive rate (1-specificity). Negative labels are not commonly accounted for. Standard regression is typically discussed in terms of explained variance, so continuous measures such as MSE and F-test are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Write a concise description (including mathematical formulae) of each of the elements for Naive Bayes:\n",
    "\n",
    "* Hypothesis\n",
    "* Cost\n",
    "* Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "* Hypothesis: We may apply Bayes' law to classify points (documents) among M different classes using N different features as keys, using the probability of  observing a given feature $x_i$ for a given class $C_{k}$, $p(x_i|C_{k})$ as if it was completely independent of the presence of other features.\n",
    "* Cost: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Why is Naive Bayes so naive? Use the mathematics to provide an explanation. The best answer will discuss the probability theory involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    The probability of observing a class $C$ dependent on the observation of a set of features $X_{1}, X_{2},...,X_{N}$ is the fully joint probability:\n",
    "    \n",
    "$$P(C|X_{1}, X_{2},...,X_{N})$$\n",
    "    \n",
    "    Applying Bayes Law we get:\n",
    "    \n",
    "$$P(C|X_{1}, X_{2},...,X_{N}) = \\dfrac{P(X_{1}, X_{2},...,X_{N}|C)P(C)}{P(X_{1}, X_{2},...,X_{N})}$$\n",
    "    \n",
    "    In order to compute the actual probability on the left, we would need to use and parameterize joint histograms with a dimension for each of the features (N dimensions). For any significant number of features (say 8 or more), an enormous training set would be required to produce adequate sampling. We can overcome this by using an assumption that the joint probabilities are independent. Two events A and B are independent iff:\n",
    "   \n",
    "   $$P(A \\cap B) = P(A,B) = P(A)P(B)$$\n",
    "    \n",
    "    This reduces our above equation to the product:\n",
    "    \n",
    "$$P(C|X_{1}, X_{2},...,X_{N}) = \\dfrac{P(X_{1}|C) P(X_{2}|C) \\cdots P(X_{N}|C)P(C) }{P(X_{1})P(X_{2}) \\cdots P(X_{N})}$$\n",
    "\n",
    "    In practice the denominator is simply a constant that is ignored, thus we have:\n",
    "    \n",
    "$$P(C|X_{1}, X_{2},...,X_{N}) = P(X_{1}|C) P(X_{2}|C) \\cdots P(X_{N}|C)P(C)$$\n",
    "    \n",
    "    However the notion that all features are completely naive, considering that pairs or triples of features are quite likely to appear due to having the same underlying cause, for example brown eyes and brown hair. It turns out that having perfect priors is less important with enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Why would I prefer to use lasso regression instead of ridge regression? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    Lasso regression is commonly used as an exploratory technique to determine which variables among many are explanatory. Normally it is used when it is believed that all variables are weakly collinear and there are several dominating variables.\n",
    "    Ridge regression is not intended to remove variables, but simply provide good shrinkage and robustness to the final regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics\n",
    "\n",
    "These questions will require a little extra writing than the above section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why does the Bias-Variance tradeoff occur? More specifically, what causes it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    The B-V tradeoff occurs due to overfitting of the model to the underlying system. Additional paramters enable the model to better capture underlying behavior of the system (less biased), but the parameters become progressively more fragile and prone to undersampling and error in the training data. Consequently the model becomes less robust (increased variance). Too few parameters creates a model that fails to capture the complexity of the undelying system, but can do so reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) How do we know that a model is underfit? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    \n",
    "    This question definitely involves some nuance, however underfitting can be detected in the ability of the model to reproduce similar measures of performance during Cross validation. In general underfitting is characterized by an inability of the model to produce good measures of performance on training data. Thus being underfit suggests a lack of sufficient model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How do we know that a model is overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "    Cross-validation can often help in this case too. Overfitting regularly produces highly accurate training results, but the model will fail when used on test data because it imputes behaviors to the system that don't exist (often a result of too many parameters rather than too few, i.e. trying to fit  2nd order polynomial with a 8th order polynomial). Measures of model performance will vary widely based on the training set used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) How do bias and variance relate to precision and accuracy? How do they relate to Type I and Type II errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "        Increased bias leads to reduced accuracy - the model cannot accurately predict the correct behavior of the system. Increased variance leads to reduced precision. Although the model may be able to predict the correct behavior of the system, it cannot do so reliably. Bias and variance can lead to both types of errors. In the case of bias we typically focus on type I error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Why is $R^{2}$ not a good metric to compare two regression models? What would be a better choice to compare them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    \n",
    "    $R^2$ ultimately is simply a measure of how the goodness-of-fit to the regressed data for an individual model, and thus does not discuss the explanatory value of additional degrees of freedom. A better measure is the F-statistic that does this explicitly. Adjusted $R^2$ can also be used, but cannot be used to ignore model-specific F-tests and p values.\n",
    "    \n",
    "    \n",
    "[read this link](https://sakai.duke.edu/access/content/group/25e08a3d-9fc4-41b0-a7e9-815732c1c4ba/New%20folder/Stat%20Topic%20Files/Non-Linear%20Regression/FTestTutorial.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 6-8\n",
    "\n",
    "Below is a ROC curve detailing the confusion matrix of a binary ultrasound test for uterine cancer. The ultrasound device makes it possible to measure its thickness. The idea of the test is to measure the thickness of the wall and set a threshold of thickness that is \"normal\". \n",
    "\n",
    "When the test measures a thickness greater this number, the patient will be labeled as a cancer risk and the test will be \"positive\". Below the number the test will be \"negative.\"\n",
    "\n",
    "Of course there are limitations. \n",
    "\n",
    "![uterine_roc](./images/ROC_endometrial.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Write a *brief* description of what is happening in the above figure, discussing the role of sensitivity and specificity with respect to the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    As the threshold is lowered, both the true positive (sensitivity) and false positive (1-specificity) (Type I error) rates are increasing. The ROC curve provides us a comparison of relative rate of increase vs. threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Suppose that this test is expensive and lacks predictive value of outcome and thus we want to minimize the number of false positives while still keeping the test usefully diagnostic. Where should we set the threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "    \n",
    "    A threshold of about 12-15 mm will be adequate. This provides a 10-20% FP rate while capturing 75-85% of the TP. This is the best setting for a test of this type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Suppose that this test is cheap and it doesn't matter to us what the human implications are of giving millions of women a year the belief that they are soon to be diagnosed with uterine cancer. Our intent is to diagnose every possible case while keeping the number of false positives reasonable. Where should we set the threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    Set the threshold around 4-5 mm. This captures nearly 100% of the TP, keeping FP around 40%. Unfortunately, this also yields a significant Type II rate, meaning there will be a large fraction of women who actually have uterine cancer (with a uterine wall thickness below the threshold) who will be later diagnosed at an unfavorable point, falsely believing they had nothing to worry about. This is a problem with overconfidence in a test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Examination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bayes\n",
    "\n",
    "You will need to use the original lab for Naive Bayes.\n",
    "\n",
    "Naive Bayes has numerous variants. The only difference between each of these variants and Multinomial Bayes is in the calculation of the distribution of likelihoods.  For Bernoulli Bayes we have the following likelihood function:\n",
    "\n",
    "$$p(y|x) = p(x_{i}|y)^{N_{1}}(1-p(x_{i}|y))^{N_{0}}$$\n",
    "\n",
    "Where $y$ is the class to be predicted, $p(x_{i}|y)$ is the frequency of a feature (word) given the class. However, we account for the prediction of the class discussing the frequencies of appearance each label has. That is to say, $N_{1}$ is the number of counts of a given feature (word) within that class and/or prediction. $N_{0}$ is the number of counts when the given feature does *not* appear within a given class! Hence we are predicting *negative* relationships within a class as well as positive relationships.\n",
    "\n",
    "you are permitted to study this [resource](http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html) and compare with [this one](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html). Don't waste too much time reading!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) Write the log-likelihood cost function needed for BernoulliBayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y|x) = \\sum_{i}^{K} N_{1, x_{i}}\\ log{(p(x_{i}|y))} + N_{0, x_{i}} log{(1-p(x_{i}|y))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Inherit from the `NaiveBayes` class from the lab to create a `BernoulliBayes` subclass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Overwrite the `_predict()` function of the `NaiveBayes` parent class (within the subclass) to reflect the log-likelihood of the Bernoulli Bayes variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "class NaiveBayes(object):\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.prior = {}\n",
    "        self.per_feature_per_label = {}\n",
    "        self.feature_sum_per_label = {}\n",
    "        self.likelihood = {}\n",
    "        self.posterior = {}\n",
    "        self.alpha = alpha\n",
    "        self.p = None\n",
    "\n",
    "    def compute_prior(self, y):\n",
    "        for label in y:\n",
    "            if label in self.prior:\n",
    "                self.prior[label] += 1\n",
    "            else:\n",
    "                self.prior[label] = 1\n",
    "\n",
    "    def compute_likelihood(self, X, y):\n",
    "        for label, row_features in zip(y, X):\n",
    "            if label in self.per_feature_per_label:\n",
    "                self.per_feature_per_label[label] += row_features\n",
    "            else:\n",
    "                self.per_feature_per_label[label] = row_features\n",
    "\n",
    "            if label in self.feature_sum_per_label:\n",
    "                self.feature_sum_per_label[label] += sum(row_features)\n",
    "            else:\n",
    "                self.feature_sum_per_label[label] = sum(row_features)\n",
    "\n",
    "        for label, per_feature_per_label_arr in self.per_feature_per_label.iteritems():\n",
    "            feature_sum_per_label = self.feature_sum_per_label[label]\n",
    "            numerator = per_feature_per_label_arr + self.alpha\n",
    "            denominator = feature_sum_per_label + self.alpha * self.p\n",
    "            self.likelihood[label] = numerator / denominator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.p = X.shape[1]\n",
    "        self.compute_prior(y)\n",
    "        self.compute_likelihood(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            \"\"\"max_label = None\n",
    "            max_value = None\"\"\"\n",
    "            labelfind = defaultdict(float)\n",
    "            for label, prior in self.prior.iteritems():\n",
    "                \n",
    "                labelfind[label] = np.log(prior) + sum([e for e in (row * np.log(self.likelihood[label]))])\n",
    "              \n",
    "                \"\"\"if max_label is None:\n",
    "                    max_label = label\n",
    "                    max_value = value\n",
    "                else:\n",
    "                    if value > max_value:\n",
    "                        max_label = label\n",
    "                        max_value = value\n",
    "            predictions.append(max_label)\"\"\"\n",
    "            predictions.append(sorted(labelfind, key=labelfind.get, reverse=True)[0])\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return sum(self.predict(X) == y) / float(len(y))\n",
    "    \n",
    "class BernoulliBayes(NaiveBayes):\n",
    "    from sklearn.preprocessing import binarize\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        NaiveBayes.__init__(self, alpha)\n",
    "   \n",
    "\n",
    "    def compute_likelihood(self, X, y):\n",
    "        for label, row_features in zip(y, X):\n",
    "            if label in self.per_feature_per_label:\n",
    "                self.per_feature_per_label[label] += binarize(row_features)\n",
    "            else:\n",
    "                self.per_feature_per_label[label] = binarize(row_features)\n",
    "\n",
    "            if label in self.feature_sum_per_label:\n",
    "                self.feature_sum_per_label[label] += sum(binarize(row_features))\n",
    "            else:\n",
    "                self.feature_sum_per_label[label] = sum(binarize(row_features))\n",
    "\n",
    "        for label, per_feature_per_label_arr in self.per_feature_per_label.iteritems():\n",
    "            feature_sum_per_label = self.feature_sum_per_label[label]\n",
    "            numerator = per_feature_per_label_arr + self.alpha\n",
    "            denominator = feature_sum_per_label + self.alpha * len(set(y))\n",
    "            self.likelihood[label] = numerator / denominator\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            row_counts = binarize(row, threshold=0.)\n",
    "            labelfind = defaultdict(float)\n",
    "            for label, prior in self.prior.iteritems():\n",
    "                log_prob = np.log(self.likelihood[label])\n",
    "                log_neg_prob = np.log(1-self.likelihood[label])\n",
    "                labelfind[label] += np.log(prior)\n",
    "                labelfind[label] += row_counts.dot(log_prob-log_neg_prob)\n",
    "                labelfind[label] += log_neg_prob.sum()\n",
    "            predictions.append(sorted(labelfind, key=labelfind.get, reverse=True)[0])\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Run your `BernoulliBayes` model on the same test harness used in the lab.  How does it score compared to the Multinomial Naive Bayes? Why would this behavior occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "        This section is only scored for logical participation. We might expect that BB performs better compared to NB due to the fact that groups (pairs) of words may be a useful indicator in this dataset, i.e. \"Nigeria\" and \"money\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3450, 57)\n",
      "Test shape: (1151, 57)\n",
      "My Implementation:\n",
      "Accuracy: 0.913119026933\n",
      "sklearn's Implementation\n",
      "Accuracy: 0.891398783666\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ec1c5dfd2034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Assert I get the same results as sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# (will give an error if different)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_predictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmy_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from naive_bayes import NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "data = np.genfromtxt('./data/spam.csv', delimiter=',')\n",
    "\n",
    "y = data[:, -1]\n",
    "X = data[:, 0:-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print 'Train shape:', X_train.shape\n",
    "print 'Test shape:', X_test.shape\n",
    "\n",
    "print \"My Implementation:\"\n",
    "my_nb = BernoulliBayes()\n",
    "my_nb.fit(X_train, y_train)\n",
    "print 'Accuracy:', my_nb.score(X_test, y_test)\n",
    "my_predictions =  my_nb.predict(X_test)\n",
    "\n",
    "print \"sklearn's Implementation\"\n",
    "mnb = BernoulliNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "print 'Accuracy:', mnb.score(X_test, y_test)\n",
    "sklearn_predictions = mnb.predict(X_test)\n",
    "\n",
    "# Assert I get the same results as sklearn\n",
    "# (will give an error if different)\n",
    "assert np.all(sklearn_predictions == my_predictions)                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
