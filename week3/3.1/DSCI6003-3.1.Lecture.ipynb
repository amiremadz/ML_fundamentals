{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 3.1 Lecture: Review for Skills Test 1\n",
    "\n",
    "## By the end of this lecture you will:\n",
    "1. Be familiar with the material covered in Skills Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "\n",
    "####What is machine learning?\n",
    "\n",
    "* Machine learning is a mapping from a training set of data $X, y$ characterizing the processes of an underlying system onto the parameters of a computational model designed to replicate the behavior of that system.\n",
    "\n",
    "Types of models:\n",
    "* Linear Models\n",
    "* Supervised Models\n",
    "* Unsupervised Models\n",
    "\n",
    "####Expected outcome results of training a model\n",
    "What happens to the prediction as we increase the size of the training set?\n",
    "\n",
    "####The unreasonable effectiveness of additional data\n",
    "What is the result of increasing the amount of training data and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2\n",
    "\n",
    "###The KNN algorithm\n",
    "\n",
    "Be able to write the KNN algorithm.\n",
    "\n",
    "* Hypothesis\n",
    "* Cost \n",
    "* Optimization\n",
    "\n",
    "####KNN weaknesses and use cases\n",
    "How does the curse of dimensionality affect KNN?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hypothesis:A point can be classified by the neighbours it is nearest to.\n",
    "    \n",
    "Cost : select class label\n",
    "    \n",
    "Optimization : Training data is learned once.\n",
    "    \n",
    "Relationship between norm and K nearest neighbour : Lp norms Almost use eucilidean or L2.\n",
    "\n",
    "Curse of dimensionality : go from 2-> 100 . Most of the points are on surface. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "\n",
    "#### The curse of dimensionality\n",
    "Consider $N$ uniformaly distributed data points  in $d$ dimensional space. If we construct a hypercube by covering a fraction $r$ of range of each feature, then number of points captured by this hypercube is given by\n",
    "\n",
    "$$n = N r^d$$\n",
    "\n",
    "If we wish to keep sampling consistent when changing from $n$ dimensions to $m$ dimensions (keeping the radius constant) we would need to take the change of radius into consideration. In general if $N^{n}$ samples are sufficient with $n$ dimensions, then we need $N^{m}$ samples in $m$ dimensions.\n",
    "\n",
    "####PCA and SVD for dimensionality reduction\n",
    "\n",
    "What is the quintessential difference between these two methods? How do we use them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Difference between PCA and SVD : In PCA we are decomposing covariance matrix . It provides eigen vector in feature space.\n",
    "    \n",
    "    SVD provides eigen vectors in feature and label space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "* Hypothesis\n",
    "* Cost (MAP not MLE) There is a [useful set of slides available on this issue](http://www.cs.cmu.edu/~tom/10601_sp08/slides/recitation-mle-nb.pdf)\n",
    "* Optimization\n",
    "\n",
    "#### Laplace smoothing: What is it and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HYpothesis :Contrbuting members of class C  are independent.\n",
    "    \n",
    "Cost function : LOG LIKELIHOOD : J(C|Theta)\n",
    "        \n",
    "Optimization : Just train the model. Training done only once. Although you can add to priors if you want to adapt.\n",
    "    Not always recommended.\n",
    "\n",
    "\n",
    "Laplace Smoothing :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "\n",
    "**Focus on this lecture**\n",
    "\n",
    "#### (K-Fold) Cross Validation\n",
    "What is it and why do we do it?\n",
    "\n",
    "#### Regression - Multilinear and Polynomial\n",
    "How do you use sklearn to fit a linear system and a polynomial? What are the differences?\n",
    "\n",
    "#### Bias and Variance - Metrics of Model Success\n",
    "1. The bias-variance tradeoff. Dartboard diagram of B-V\n",
    "2. Confusion matrix.\n",
    "3. Type I and type II errors. \n",
    "4. Formulas and interpretation of precision, recall, and F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K-Fold cross val : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 \n",
    "\n",
    "#### The 6 assumption of OLS\n",
    "\n",
    "####Regularized regression \n",
    "1. The role of norms in RR\n",
    "2. Interpretation of the squid diagram\n",
    "3. The difference in use cases between Lasso, Ridge and eNet\n",
    "4. Formulation of regularized cost functions\n",
    "\n",
    "####Regression elements:\n",
    "1. Hypothesis\n",
    "2. Cost\n",
    "3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis : A linear relationship among the weights and thus variables.\n",
    "    \n",
    "Cost : SSE - f(size(beta))\n",
    "    \n",
    "Optimization : OLS automatically optimized by $(X^{T}X)^{-1}Xy=\\beta$ \n",
    "Descend the cost function.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "\n",
    "#### Logistic Regression\n",
    "1. Hypothesis : In a class or not - relationship betwee\n",
    "2. Cost - MAP, not MLE in this case.\n",
    "3. Optimization : gradient descent\n",
    "\n",
    "what is the meaning of regression weight.\n",
    "\n",
    "#### Regularized Regression\n",
    "\n",
    "#### Generative vs. Discriminative models : \n",
    "\n",
    "Generative predicts joint probability, doesnt requires a input\n",
    "discriminative predicts conditional. requires a input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "#### Precision vs. Recall\n",
    "\n",
    "#### ROC Curves Sensitivity vs. (1-Specificity) (true positive vs. false positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
