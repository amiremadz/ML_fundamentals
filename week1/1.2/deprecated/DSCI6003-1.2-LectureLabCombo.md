#_K_ Nearest  Neighbors Classifier

##Classification Problem

The task of a classification algorithm or _classifier_ is to predict the _label_ or _class_ for a given unlabeled data point. Mathematically, a classifier is a function or model $f$ that predicts the class label  for a given input example $\bf x$, that is

$$\hat{y} = f({\bf x})$$

The value $\hat{y}$ belongs to a set $\{c_1,c_2,...,c_k\}$ and each $c_i$ is a class label.

To build the model we require a set of points with known class labels, which is called a _training set_. Once the model $f$ is known, we can automatically predict the class for any new data point.

####Fisher's Iris Data Set

Following table shows an extract of the Iris dataset; the complete data forms a $150\times 4$ data matrix. Each entity is an Iris flower, and the attributes include _sepal length_, _sepal width_, _petal length_, and _petal width_ in centimeters, and the type or class of the Iris flower.

<img src="imgs/iris_data_table.png" width=350/>

The classifier algorithm will _learn_ from this data and predict the class of new data set with unknown class.

##_K_ Nearest  Neighbors

For fixed $K$, _the KNN classifier predicts the class of $\bf x$ as the majority class among its $K$ nearest neighbors_.

<img src="imgs/knn_cartoon.png"/ width=300>

$K$ is a hyperparamter for the claissifier. The predicted class can be different for different values of $K$. For binary classification tasks, odd values of $K = 1,3,5,\cdots$ are used to avoid ties, i.e., two class labels achieving the same score. For more than two classes, ties can be broken at random. The particular case of $K = 1$ is called the _nearest-neighbour_ rule, because a test point is simply assigned to the same class as the nearest point from the training set.

###Distance Metrics

As explained above, KNN algorithm assigns a class to the test point based on the majority class of $K$ nearest neighbours. In general, euclidean distance is used to find nearest neighbours, but other distance metrics can also be used.

As the dimensionality of the feature space increases, the euclidean distance becomes problematic due to _curse of dimensionality_ (discussed later). In such cases, typically vector-based similarity measures (dot product, cosine similarity, etc) are used to find nearest neighbours.

###Algorithm

KNN algorithm is very simple to implement. The model does not need to be trained. We just need to store the data. For a test data point, we calculate the distance of that data point to every existing data point and find the $K$ closest ones. Here's the pseudocode for _K_ Nearest Neighbors:

```
kNN:
    for every point in the dataset:
        calculate the distance between the point and x
        sort the distances in increasing order
        take the k items with the smallest distances to x
        return the majority class among these k items
```

Note that, for large data sets, the algorithm can take very long to classify becuase it has to calculate the distance between the test point and evey other point in the data!

###Accuracy and Error Rate

We can assess the performance of a classifier by looking at the _error rate_ and _accuracy_ which are defined as follows.

The error rate is the fraction of incorrect predictions over the testing set. Mathematically, we can express this as

$$\text{Error Rate } = \frac{1}{n} \sum_{i=1}^n I(y_i \ne \hat{y}_i)$$

where $I$ is an _indicator function_ that has the value $1$ when its argument is true, and $0$ otherwise.

The accuracy of a classifier is the fraction of correct predictions over the testing set:

$$\text{Accuracy } = \frac{1}{n} \sum_{i=1}^n I(y_i = \hat{y}_i) = 1 - \text{Error Rate}$$

##KNN Implementation

For this exercise, we'll use Fisher's Iris Data Set:

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
...
>>> #print iris.DESCR
```

```python
>>> X = iris.data
>>> y = iris.target
>>> y_name = iris.target_names
...
>>> #print X[:5]
... #print y_name
```

```python
>>> import matplotlib.pyplot as plt
>>> %matplotlib inline
...
>>> #plt.plot(X[:,0], X[:,1], '.');
```

##Exercise

- Implement the class `KNearestNeighbors`. We are going to write our code similar to how sklearn does. So you should be able to run your code like this:
```python
knn = KNearestNeighbors(k=3, distance=euclidean_distance)
knn.fit(X, y)
y_predict = knn.predict(X)
```

- Implement the function `euclidean_distance` which computes the Euclidean distance between two numpy arrays. Euclidean distance only works for continuous features.

$$dist({\bf a}, {\bf b}) = \| {\bf a} - {\bf b} \| = \sqrt{({\bf a} - {\bf b})^T ({\bf a} - {\bf b})}$$

```python
>>> def euclidean_distance(a, b):
...     pass
```

- Implement `cosine_distance` which computes the cosine similarity between the two vectors.

$$dist({\bf a}, {\bf b}) = \frac{{\bf a}^T {\bf b}}{\| {\bf a} \| \| {\bf b} \|}$$

```python
>>> def cosine_distance(a, b):
...     pass
```

```python
>>> class KNearestNeighbors(object):
...     def __init__(self, k=5, distance=euclidean_distance):
...         pass
...
...     def fit(self, X, y):
...         pass
...
...     def classify(self, x):
...         pass
...
...     def predict(self, X):
...         pass
```

```python
>>> def error_rate(y_test, y_pred):
...     pass
...
>>> def accuracy(y_test, y_pred):
...     pass
```
