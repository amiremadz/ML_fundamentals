{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gradient Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the funtion $f({\\bf x})$ where ${\\bf x} \\in R^d$. The gradient is the derivative of $f$ w.r.t $\\bf x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f({\\bf x}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\left[ \\frac{\\partial f}{\\partial x_1} ~~~ \\frac{\\partial f}{\\partial x_2} ~~~ \\cdots ~~~ \\frac{\\partial f}{\\partial x_d} \\right]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If $f$ is differentiable and has an extremum at a point ${\\bf x}$ in a region $S$, then the partial derivatives $\\partial f/\\partial x_j$ must be zero at $\\bf x$. These are the components of the gradient of $f$. Thus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f({\\bf x}) = {\\bf 0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point at which the gradient is zero, is called a stationary point of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Regression Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that at discrete points ${\\bf x}_i$, observations $y_i$ of some phenomenon are made, and the results are recorded as a set of ordered pairs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\{({\\bf x}_1, y_1), ({\\bf x}_2, y_2), \\cdots, ({\\bf x}_n, y_n)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the basis of these points, the problem is to make estimations or predictions at points $\\hat{{\\bf x}}$ that are between or beyond the observation points ${\\bf x}_i$. We can try to find the equation of a curve $y=f({\\bf x})$ that closeley fits the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest curve to fit the data is a hyperplane:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f({\\bf x}) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_d x_d = {\\bf \\beta}^T {\\bf x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ${\\bf \\beta} = [\\beta_0 ~~ \\beta_1 ~~ \\cdots ~~ \\beta_d]^T$, and we've used $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/least_squares_plane.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine the coefficients $\\beta$ that best fit the data in the sense that the sum of the squares of the errors $e_1, e_2, \\cdots, e_n$ is minimal. The distance from the point $({\\bf x}_i, y_i)$ to the plane $f({\\bf x})$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_i = \\left |f({\\bf x}_i) - y_i \\right| = \\left |\\beta^T {\\bf x}_i - y_i \\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define a vector ${\\bf e} = [e_1 ~~ e_2 ~~ \\cdots ~~ e_d]^T$, we can write the __cost function__ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J({\\bf \\beta}) = {\\bf e}^T {\\bf e} = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (\\beta^T {\\bf x}_i - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to find the the coefficients $\\bf \\beta$ that minimize the above const function. The condition for minima is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla J({\\bf \\beta}) = \\frac{\\partial J}{\\partial {\\bf \\beta}} = {\\bf 0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use gradient descent method to find $\\bf \\beta$ that minimizes the cost function. This is an uncontrained optimization of $J({\\bf \\beta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $J$ has a minimum at ${\\hat{\\bf \\beta}}$. We start at some $\\bf \\beta$ (close to $\\hat{\\bf \\beta}$) and we look for the minimum of $J$ close to $\\bf \\beta$ along the straight line in the direction of $-{\\bf \\nabla} J({\\bf \\beta})$, which is the direction of steepest descent (= direction of maximum decrease) of $J$ at $\\bf \\beta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf \\beta}'(\\alpha) = {\\bf \\beta} - \\alpha {\\bf \\nabla} J({\\bf \\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take ${\\bf \\beta}'$ as our next approximation to ${\\bf \\beta}$. $\\alpha$ is called the _learning rate_.\n",
    "\n",
    "In order for gradient descent to work we must set the learning rate to an appropriate value. This parameter determines how fast or slow we will move towards the optimal value. If the $\\alpha$ is very large we will skip the optimal solution. If it is too small we will need too many iterations to converge to the best values.\n",
    "\n",
    "<img src=\"imgs/learning_rate.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the gradient descent method, we first need to compute the partial derivatives fo $J$ w.r.t to components of $\\bf \\beta$. The derivative with respect to the $j$th component of $\\bf \\beta$ can be evaluated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j}\\left( \\sum_{i=1}^{n} (\\beta^T {\\bf x}_i - y_i)^2 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the derivative is a linear operation, we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_j}(\\beta^T {\\bf x}_i - y_i)^2~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$~~~~~~~~~~~~~~~~~~~~~~ = \\sum_{i=1}^{n} 2 (\\beta^T {\\bf x}_i - y_i) \\frac{\\partial}{\\partial \\beta_j}(\\beta_0 x_{i0} + \\beta_1 x_{i1} +\\cdots +\\beta_d x_{id} - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{i=1}^{n} 2 (\\beta^T {\\bf x}_i - y_i) x_{ij}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express the above derivative in matrix form by represent the data in terms of the following matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf X} = \\begin{bmatrix} {\\bf x}_1^T \\\\ {\\bf x}_2^T \\\\ \\vdots \\\\ {\\bf x}_n^T \\end{bmatrix} = \\begin{bmatrix} {\\bf X}_0 & {\\bf X}_1 & \\cdots & {\\bf X}_d \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ${\\bf X}_j$ denote _feature vectors_. Thus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf X \\beta} - {\\bf y} = \\begin{bmatrix} {\\bf x}_1^T {\\bf \\beta} - y_1 \\\\ {\\bf x}_2^T {\\bf \\beta} - y_2 \\\\ \\vdots \\\\ {\\bf x}_n^T {\\bf \\beta} - y_n\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ${\\bf \\beta}^T {\\bf x}_i = {\\bf x}_i^T {\\bf \\beta}$. This allows us to express the derivatives of $J$ in more compact form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\beta_j} = 2 {\\bf X}_j^T ({\\bf X \\beta} - {\\bf y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla J({\\bf \\beta}) = \\left[ \\frac{\\partial J}{\\partial \\beta_0} ~~ \\frac{\\partial J}{\\partial \\beta_1} ~~ \\cdots ~~ \\frac{\\partial J}{\\partial \\beta_d} \\right]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, the cost function becomes\n",
    "\n",
    "$$J({\\bf \\beta}) = ({\\bf X \\beta} - {\\bf y})^T ({\\bf X \\beta} - {\\bf y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider the data collected in an observational study in a semiconductor manufacturing plant. The file `pull_strength.csv` contains three variables, pull strength (a measure of the amount of force required to break the bond), the wire length, and the height of the die. Find a model relating pull strength to wire length and die height. Plot the data and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.    2.   50.]\n",
      " [   1.    8.  110.]\n",
      " [   1.   11.  120.]\n",
      " [   1.   10.  550.]\n",
      " [   1.    8.  295.]\n",
      " [   1.    4.  200.]\n",
      " [   1.    2.  375.]\n",
      " [   1.    2.   52.]\n",
      " [   1.    9.  100.]\n",
      " [   1.    8.  300.]\n",
      " [   1.    4.  412.]\n",
      " [   1.   11.  400.]\n",
      " [   1.   12.  500.]\n",
      " [   1.    2.  360.]\n",
      " [   1.    4.  205.]\n",
      " [   1.    4.  400.]\n",
      " [   1.   20.  600.]\n",
      " [   1.    1.  585.]\n",
      " [   1.   10.  540.]\n",
      " [   1.   15.  250.]\n",
      " [   1.   15.  290.]\n",
      " [   1.   16.  510.]\n",
      " [   1.   17.  590.]\n",
      " [   1.    6.  100.]\n",
      " [   1.    5.  400.]]\n",
      "beta:\n",
      "[[ 2.26379143]\n",
      " [ 2.74426964]\n",
      " [ 0.01252781]]\n",
      "\n",
      "RSS:  115.17348278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "data = np.genfromtxt('pull_strength.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "y = data[:,0:1]\n",
    "X = data[:,1:3]\n",
    "\n",
    "A = np.hstack((np.ones([y.shape[0],1]), X))\n",
    "\n",
    "print A\n",
    "\n",
    "b = np.linalg.inv(A.T.dot(A)).dot(A.T.dot(y))\n",
    "print 'beta:\\n', b\n",
    "\n",
    "e = A.dot(b) - y #error vector\n",
    "print \"\\nRSS: \", e.T.dot(e)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can solve the same problem using gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:\n",
      "[[ 2.02227938]\n",
      " [ 2.57081564]\n",
      " [ 0.01688604]]\n",
      "\n",
      "RSS:  138.917910182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x106fd9c50>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAG1NJREFUeJzt3Xu4HXV97/H3B0LkWpIUzR1IaVLIUSu3oIJl44EAFkO8\n",
       "nBDbaipan5ZHBU+PJsFTydOLhRy1iJVWrdKUkkgexDSRWxJg16JCuAQMhJBEjWQHsxOuomhNyPf8\n",
       "8fvt7JWdnb3XTtbas9aaz+t55tkzs2bWfNdA1mf9Zn4zo4jAzMzK66CiCzAzs2I5CMzMSs5BYGZW\n",
       "cg4CM7OScxCYmZWcg8DMrOT6DQJJcyU9IWmNpIWSXiNphKQVktZLWi5pWI/lN0haJ2lqfcs3M7MD\n",
       "pb6uI5B0PHAPcFJE/Lekm4Hbgf8BPBsR8yXNBoZHxBxJk4GFwOnAWGAlMCkidtX3Y5iZ2f7qr0Xw\n",
       "c2AHcLikIcDhwDPANGBBXmYBMD2PXwwsiogdEbEJ2AhMqXXRZmZWO30GQUQ8D3weeJoUAC9GxApg\n",
       "ZER05sU6gZF5fAzQUfEWHaSWgZmZNag+g0DSCcAVwPGkL/kjJf1J5TKRji31dZ8K38PCzKyBDenn\n",
       "9dOA70fEcwCSbgXeAmyVNCoitkoaDWzLy28BxlesPy7P24Mkh4OZ2X6ICNX6PfsLgnXAX0k6DPg1\n",
       "cC6wCvglMAu4Jv9dkpdfCiyU9AXSIaGJefm91OPDNCNJ8yJiXtF1NALvi27eF928L7rV60d0n0EQ\n",
       "EY9J+jfgIWAX8AjwVeAoYLGkDwGbgBl5+bWSFgNrgZ3AZeHbm5qZNbT+WgRExHxgfo/Zz5NaB70t\n",
       "/1ngswdempmZDQZfWVy89qILaCDtRRfQQNqLLqCBtBddQKvr84Kyum1UCp8jMDMbmHp9d7pFYGZW\n",
       "cg4CM7OScxCYmZWcg8DMrOQcBGZmJecgMDMrOQeBmVnJOQjMzErOQWBmVnIOAjOzknMQmJmVnIPA\n",
       "zKzkHARmZiXnIDAzKzkHgZlZyTkIzMxKzkFgZlZyDgIzs5JzEJiZlZyDwMys5BwEZmYl128QSPo9\n",
       "SasrhpckfVzSCEkrJK2XtFzSsIp15kraIGmdpKn1/QhmZnYgFBHVLywdBGwBpgAfA56NiPmSZgPD\n",
       "I2KOpMnAQuB0YCywEpgUEbsq3iciQjX8HGZmLa9e350DPTR0LrAxIjYD04AFef4CYHoevxhYFBE7\n",
       "ImITsJEUHGZm1oAGGgQzgUV5fGREdObxTmBkHh8DdFSs00FqGZiZWQMaUu2CkoYC7wRm93wtIkJS\n",
       "X8eY9npN0ryKyfaIaK+2FjOzMpDUBrTVeztVBwFwIfBwRGzP052SRkXEVkmjgW15/hZgfMV64/K8\n",
       "PUTEvP2o18ysNPIP5PauaUlX1WM7Azk09D66DwsBLAVm5fFZwJKK+TMlDZU0AZgIrDrQQs3MrD6q\n",
       "6jUk6Qjgp8CEiHg5zxsBLAaOBTYBMyLixfzalcClwE7g8oi4q8f7udeQmdkA1eu7c0DdR2u2UQeB\n",
       "mdmANUr3UTMzazEOAjOzknMQmJmVnIPAzKzkHARmZiXnIDAzKzkHgZlZyTkIzMxKzkFgZlZyDgIz\n",
       "s5JzEJiZlZyDwMys5BwEZmYl5yAwMys5B4GZWck5CMzMSs5BYGZWcg4CM7OScxCYmZWcg8DMrOQc\n",
       "BGZmJecgMDMrOQeBmVnJVRUEkoZJukXSk5LWSjpD0ghJKyStl7Rc0rCK5edK2iBpnaSp9SvfzMwO\n",
       "VLUtgi8Ct0fEScAbgXXAHGBFREwC7s7TSJoMXAJMBi4ArpfkloeZWYPq9wta0tHA2yLiGwARsTMi\n",
       "XgKmAQvyYguA6Xn8YmBRROyIiE3ARmBKrQs3M7PaqOaX+gRgu6QbJD0i6WuSjgBGRkRnXqYTGJnH\n",
       "xwAdFet3AGNrVrGZmdXUkCqXOQX4aEQ8KOla8mGgLhERkqKP99jrNUnzKibbI6K9ilrMzEpDUhvQ\n",
       "Vu/tVBMEHUBHRDyYp28B5gJbJY2KiK2SRgPb8utbgPEV64/L8/YQEfP2u2ozsxLIP5Dbu6YlXVWP\n",
       "7fR7aCgitgKbJU3Ks84FngCWAbPyvFnAkjy+FJgpaaikCcBEYFVNqzYzs5qppkUA8DHgJklDgR8B\n",
       "HwQOBhZL+hCwCZgBEBFrJS0G1gI7gcsioq/DRmZmViAV8R0tKSJCg75hM7MmVq/vTvfvNzMrOQeB\n",
       "mVnJOQjMzErOQWBmVnIOAjOzknMQmJmVnIPAzKzkHARmZiXnIDAzKzkHgZlZyTkIzMxKzkFgZlZy\n",
       "DgIzs5JzEJiZlZyDwMys5BwEZmYl5yAwMys5B4GZWck5CMzMSs5BYGZWcg4CM7OSKywIJA4uattm\n",
       "ZtatyBbBlyRU4PbNzIwqg0DSJkk/lLRa0qo8b4SkFZLWS1ouaVjF8nMlbZC0TtLUfbztFODvHQZm\n",
       "ZsWqtkUQQFtEnBwRU/K8OcCKiJgE3J2nkTQZuASYDFwAXC+pt+1cAFwEfPoA6jczswM0kENDPX+5\n",
       "TwMW5PEFwPQ8fjGwKCJ2RMQmYCPp1/8eIngWOBf4gMQnBlK0mZnVzkBaBCslPSTpz/K8kRHRmcc7\n",
       "gZF5fAzQUbFuBzC21zcNtpLC4HKJjwyocjMzq4khVS53ZkT8TNJrgRWS1lW+GBEhKfpYf6/XJM3r\n",
       "nvrDT8N3rpF4JYJ/r7ImM7OWJqkNaKv3dqoKgoj4Wf67XdK3SYd6OiWNioitkkYD2/LiW4DxFauP\n",
       "y/N6vue8ymmJ1cDdOQxuHfAnMTNrMRHRDrR3TUu6qh7b6ffQkKTDJR2Vx48ApgJrgKXArLzYLGBJ\n",
       "Hl8KzJQ0VNIEYCKwqr/tRLAWeAfwTxLvHOgHMTOz/VNNi2Ak8G1JXcvfFBHLJT0ELJb0IWATMAMg\n",
       "ItZKWgysBXYCl0VEX4eNdotgtcRFwHck/iyCpQP+RGZmNiCq8ju6thuVIiL2ef2AxGnAbcBHIviP\n",
       "wavMzKxx9ffdub+qPVk8qCJ4SOIdwO0Sith92MnMzGqsIYMAIIKHJS4E7shh8O2iazIza0UNGwQA\n",
       "ETySw6CrZeDeRGZmNdbQQQB7hMEdEodEcHPRNZmZtZKGDwLY3ZvoPOBOiSMj+HrRNZmZtYqmCAKA\n",
       "CNZItAErJI6K4NqiazIzawVNEwQAEWyQ+ANgpcRvAX8TsfftK8zMrHpN96jKCJ4G3ga8B/icn2dg\n",
       "ZnZgmi4IACLoJN2I6Uzgq37spZnZ/mvKIACI4AXgPOAE4JsShxZckplZU2raIACI4GXSjeqC1KNo\n",
       "WD+rmJlZD00dBAAR/BqYCfwQ+K7U+0NwzMysd00fBAAR7AIuB24CvidxUsElmZk1jabqPtqX3I30\n",
       "GolngHaJd0fwvaLrMjNrdC3RIqgUwY3AB4BvS0wvuh4zs0bXckEAEMFdwIXAlyX+0tcamJntW0M+\n",
       "mKZ22+FY4DvAD4CPRrCj3ts0M6uXen13tmSLoEu+CvlMYCzp7qXuXmpm1kNLBwHsvtbgYuBx4AcS\n",
       "JxRckplZQ2n5IACI4NUIrgCuA+6TOKvomszMGkUpgqBLBP8E/Clwq8SlBZdjZtYQWvpk8b63z4nA\n",
       "EmAl8AmfRDazZuCTxTUUwTrgDOA44G6J1xVckplZYaoKAkkHS1otaVmeHiFphaT1kpZLGlax7FxJ\n",
       "GyStkzS1XoUfqAheIp1Ebgcekjit2IrMzIpRbYvgcmAt7H4a2BxgRURMAu7O00iaDFwCTAYuAK6X\n",
       "1LCtjgh2RfAZ4ApS99IPFF2Tmdlg6/dLWtI40q2e/wV2X6E7DViQxxfA7ls5XAwsiogdEbEJ2AhM\n",
       "qWXB9RDBrcA5wGckrpMYWnRNZmaDpZpf6/8AfBLYVTFvZER05vFOYGQeHwN0VCzXAc1xW+gIHgdO\n",
       "B44F/kviuIJLMjMbFH3efVTSRcC2iFgtqa23ZSIiJPXV9ajX1yTNq5hsj4j2vkutvwhekHgX8JfA\n",
       "KolLI7it6LrMrJzy925bvbfT322o3wpMk/QO4FDgtyTdCHRKGhURWyWNBrbl5bcA4yvWH5fn7SUi\n",
       "5h1Q5XWSb2f9OYkfkB6B+e/AX0Wws+DSzKxk8g/k9q5pSVfVYzt9HhqKiCsjYnxETCA9BeyeiHg/\n",
       "sBSYlRebReqTT54/U9JQSROAicCqehReb/lZBqcAp5K6mI4uuCQzs7oYaI+ersM8VwPnSVoPvD1P\n",
       "ExFrgcWkHkZ3AJdFEVes1UgE20m3s74HeFji/IJLMjOruVJeWbw/JN5O6iG1GLgygv8uuCQzKxlf\n",
       "WVywCO4B3gT8DnB/vk2FmVnTcxAMQATPAe8G/pnUxfTDfvqZmTU7HxraTxKTgUXABuAjETxfcElm\n",
       "1uJ8aKjBRLCWdOO6zcBjEg17XyUzs764RVADEucBXwduBz6Zn4pmZlZTbhE0sAhWAG8AhpJaB2cX\n",
       "XJKZWdXcIqgxiXcCXwFuJnUz/VXBJZlZi3CLoElEsIzUOhgFrJZ4c8ElmZn1yS2COpKYAXyR1Dr4\n",
       "vxH8ouCSzKyJuUXQhCJYDLweGAY8LnFhwSWZme3FLYJBknsWfQX4PvCJfB8jM7OquUXQ5Cp6Fm0l\n",
       "tQ7e76uSzawRuEVQAInTSI/+3A58NIKnCi7JzJqAWwQtJIKHSI/FvAP4nsTfSRxecFlmVlIOgoJE\n",
       "sCOCLwBvBCYAayWm+3CRmQ02HxpqEPl5B18GfgJ8LIIfFVySmTUYHxpqcfl5B79Pej7pAxJ/LXFE\n",
       "sVWZWRk4CBpIBL+JYD5wMnAC8JTELMn/ncysfnxoqIHl21NcCxxMuvbgvoJLMrMC1eu700HQ4PLJ\n",
       "4/cBVwMPAJ+K4CfFVmVmRfA5gpKKICJYCJwIPAY8KHG1xLCCSzOzFuEgaBIRvBLB35K6mx4DrJf4\n",
       "pMRhBZdmZk3OQdBkIngmgg8DZwNvJgXChyWGFFyamTWpPoNA0qGSHpD0qKS1kv4+zx8haYWk9ZKW\n",
       "SxpWsc5cSRskrZPk5/jWSQRPRvAe4L3AH5HuX/QeX5BmZgPV78liSYdHxCuShgD3Af8HmAY8GxHz\n",
       "Jc0GhkfEHEmTgYWk2yeMBVYCkyJiV4/39MniGspf/ueRTijvBD4NrIxg8HsCmFndFHayOCJeyaND\n",
       "Sd0YXyAFwYI8fwEwPY9fDCyKiB0RsQnYCEypZcG2t3xCeTlwGvB54DrgPonz3EIws/70GwSSDpL0\n",
       "KNAJ3BsRTwAjI6IzL9IJjMzjY4COitU7SC0DGwQR7IrgZtLDcP4RB4KZVaHfE4z5sM6bJB0N3CXp\n",
       "nB6vh6S+DkH0+pqkeRWT7RHR3n+5Vo0IXgUWSSwGZpAC4XmJefiQkVnTkNQGtNV7O1X3NImIlyTd\n",
       "BpwKdEoaFRFbJY0GtuXFtgDjK1Ybl+f19n7z9q9kq9Y+AuEFic8CtzkQzBpb/oHc3jUt6ap6bKe/\n",
       "XkPHdPUIknQY6YTkamApMCsvNgtYkseXAjMlDZU0AZgIrKpH4Va9CF6NYBHpkNF1wN8CP5T4Y3c7\n",
       "NbM+ew1JegPpZPBBebgxIv6fpBHAYuBYYBMwIyJezOtcCVxK6r1yeUTc1cv7utdQgfL5gvOB2aRn\n",
       "IXwO+EYEr/S5opkVyvcasrrIN7abDbwV+BLw5QheKLYqM+uN7zVkdRHB/RG8i3RC6gTgRxJfkphY\n",
       "bGVmNlgcBAbsvlL5g6TzCC+RnqW8TOJ/uuupWWvzoSHrlcThwB8DVwC7SM9FWBjBrwotzKzEfI7A\n",
       "CpFbA+eSAuE04KvAVyL2uHDQzAaBzxFYIfLtK1ZE8IfAHwDDSV1Pl0ic78domjU/twhswCSOJD01\n",
       "7S+Ao4GvADdEsL3QwsxanFsE1jAi+EUEXyNdZf4+4CTScxFukjjLJ5fNmotbBFYTEsOBDwB/nmfd\n",
       "ANwYwc+Kq8qstfhksTWF3Bp4C+nq8vcA3yOFwrIIflNkbWbNzkFgTUfiCFIYXAp0PbTohggeK7Qw\n",
       "syblILCmJnEC6QaFfwo8C9wEfDOi97vTmtneHATWEiQOJt3O4o+AdwGPkloK3/I9jsz65iCwliNx\n",
       "KPAOUiicB9xDainc5iuYzfbmILCWJnE08G7SbS1OJT3b4hZgRQS/LrI2s0bhILDSkBgN/C/SieY3\n",
       "ArcD3wLu9DMTrMwcBFZKEqOA6cB7gdOBu0ihcFsEvyiyNrPB5iCw0pM4BriYFApnks4pLCWFQmeR\n",
       "tZkNBgeBWYV8JfNFwDuBqcCTwDJSMDwRweD/j21WZw4Cs32QGAqcDUwjBUOQAmEZ8F1f0WytwkFg\n",
       "VoV8i4vX0x0KvwfcTTq3cFcETxdYntkBcRCY7Yd8snkqcAHpWoXt5FAA/tPXK1gzcRCYHaD8EJ1T\n",
       "gPNJwfAm4Pt0B8Nan1uwRuYgMKuxfBHb20nBcD5wKHAvqTfSvcCPHQzWSAoLAknjgX8DXkc6CffV\n",
       "iLhO0gjgZuA4YBMwIyJezOvMJd1x8lXg4xGxvMd7Ogis4UhMIAXDOfnvDiqCIYLNBZZnVmgQjAJG\n",
       "RcSjko4EHiZd4PNB4NmImC9pNjA8IuZI6rrd8OnAWGAlMCkidtX7w5jVSj7pPInuYDgHeJEUDP8J\n",
       "3Ac87RaDDaaGOTQkaQnwj3k4OyI6c1i0R8SJuTWwKyKuycvfCcyLiPsr3sNBYE0ln194PSkQ3gac\n",
       "RWox3FcxPB7Bq4UVaS2vXt+dQwZYxPHAycADwMiI6LqasxMYmcfHAPdXrNZBahmYNa0IdgE/zMMX\n",
       "c4vhBFIgnAV8DBgl8QO6g2GVeyVZM6g6CPJhoW8Bl0fEy1J3KEVESOqrabHXa5LmVUy2R0R7tbWY\n",
       "FS0fEtqYh38FkHgt6dYXZwJXA2+UeBJYRfrx9ACwPoeKWb8ktZGe31Hf7VRzaEjSIcB3gDsi4to8\n",
       "bx3QFhFbJY0G7s2HhuYARMTVebk7gasi4oGK9/OhIWt5EoeRWtBTgDPyMAJ4kO5gWOX7JFm1ijxZ\n",
       "LGAB8FxEfKJi/vw875r85T+sx8niKXSfLP7dqNiQg8DKSuJ1pI4UXcEwBXiJ1Gp4GHgEWB3Bs4UV\n",
       "aQ2ryCA4C/gu6dho18JzSf/jLgaOZe/uo1eSuo/uJB1KuqvHezoIzNh9Evp3SaFwCqkFcTIpHB7J\n",
       "w+r89xn3Uiq3huk1VJONOgjM9imHwwS6g+GUPATdofAosAbYEMHOgkq1QeYgMCux3EtpDHu2Gt6Q\n",
       "5z1FCoU1wOP57xa3HlqPg8DM9iJxBDCZFAqVw1D2DocnI3iuoFKtBhwEZla1fFK6KxRen/+eCPyG\n",
       "9BCfdXnoGv+pu7U2PgeBmR2QfHhpFHASKRROrBj/bWA9ewbEU8BGPxu6cTgIzKxuJI4iPcSnMhxO\n",
       "BH6HdI+ljb0NEbxUSMEl5SAws0GXezCNIXVx7W34FSkUNtAdED8mdSnv9Anr2nIQmFlDyYeaRrJn\n",
       "MEwkdX09HjgK+CkpFHobHBQD5CAws6aSezQdRwqF3oaeQfFTYDPpRpWbSV1gfz14FTc+B4GZtZR9\n",
       "BMW4imEs8HO6g6GjYuia3hLBK4NbeXEcBGZWKvn8xGvZMxzGAeN7TP+SFAw/y8PW3sZbITAcBGZm\n",
       "PeTzFMeQAmF0xTCql/Ed9B0WncA24NlGvW2Hg8DMbD/lwDiafYfEaNKJ79eSbhX+MikUtudhW4+/\n",
       "leODFhwOAjOzQZAPSY0ghcLr8t++xkeQzmV0BcRzeXg+D72NPwf8aqC9phwEZmYNSOJgYDjd4TAi\n",
       "D7/dz/hB7DsouoYXSBf05b/a4CAwM2sR+Ql2I+g7LIYDw/Lf4aAJDgIzsxKr13fnQbV+QzMzay4O\n",
       "AjOzknMQmJmVnIPAzKzkHARmZiXnIDAzK7l+g0DSNyR1SlpTMW+EpBWS1ktaLmlYxWtzJW2QtE7S\n",
       "1HoVbmZmtVFNi+AG4IIe8+YAKyJiEnB3nkbSZOASYHJe53pJbnX0QVJb0TU0Cu+Lbt4X3bwv6q/f\n",
       "L+mI+C/S5c2VpgEL8vgCYHoevxhYFBE7ImIT6bF1U2pTastqK7qABtJWdAENpK3oAhpIW9EFtLr9\n",
       "/bU+MiI683gn6a59kJ5t2lGxXAfp4RJmZtagDviwTaR7VPR1nwo/k9TMrIEN2c/1OiWNioitkkaT\n",
       "7ssNsIX09KAu4/K8vUhyQGSSriq6hkbhfdHN+6Kb90V97W8QLAVmAdfkv0sq5i+U9AXSIaGJwKqe\n",
       "K/uGc2ZmjaPfIJC0CDgbOEbSZuAzwNXAYkkfAjYBMwAiYq2kxcBaYCdwWRRxe1MzM6taIbehNjOz\n",
       "xjHoffwlXZAvNtsgafZgb7/eJI2XdK+kJyQ9Lunjef6AL8KTdKqkNfm1LxbxeWpB0sGSVktalqdL\n",
       "uS8kDZN0i6QnJa2VdEaJ98Xc/G9kjaSFkl5Tln1Rq4t09/XZ8768Oc+/X9Jx/RYVEYM2AAeTri04\n",
       "HjgEeBQ4aTBrGITPOAp4Ux4/EngKOAmYD3wqz58NXJ3HJ+f9cEjeLxvpbqmtAqbk8duBC4r+fPu5\n",
       "T/43cBOwNE+Xcl+Qrrm5NI8PIT1MvXT7In+eHwOvydM3k841lmJfAG8DTgbWVMyr2WcHLgOuz+OX\n",
       "AN/st6ZB3gFvAe6smJ4DzCn6P0ydP/MS4FxgHen6C0hhsS6PzwVmVyx/J/BmYDTwZMX8mcA/F/15\n",
       "9uPzjwNWAucAy/K80u2L/KX/417ml3FfjCD9QBpOCsRlwHll2hf5S70yCGr22fMyZ+TxIcD2/uoZ\n",
       "7ENDY4HNFdMtfcGZpONJyf8AA78Ir+f8LTTnvvoH4JPArop5ZdwXE4Dtkm6Q9Iikr0k6ghLui4h4\n",
       "Hvg88DTwDPBiRKyghPuiQi0/++7v2YjYCbwkaURfGx/sICjNmWlJRwLfAi6PiJcrX4sU1S2/LyRd\n",
       "BGyLiNVAr12Gy7IvSL/MTiE12U8Bfkm+R1eXsuwLSScAV5B+FY8BjpT0J5XLlGVf9KaIzz7YQdDz\n",
       "grPx7JlqLUHSIaQQuDEiuq6x6JQ0Kr/e30V4HXn+uB7ze704r4G9FZgm6SfAIuDtkm6knPuiA+iI\n",
       "iAfz9C2kYNhawn1xGvD9iHgu/2K9lXTYuIz7okst/k10VKxzbH6vIcDRuRW2T4MdBA8BEyUdL2ko\n",
       "6UTG0kGuoa4kCfg6sDYirq14qesiPNj7IryZkoZKmkC+CC8itgI/zz1LBLy/Yp2mEBFXRsT4iJhA\n",
       "OoZ5T0S8n3Lui63AZkmT8qxzgSdIx8dLtS9Ix8PfLOmw/BnOJV17VMZ90aUW/yb+o5f3ei/pDtF9\n",
       "K+AkyYWkE0UbgblFn7Spw+c7i3Q8/FFgdR4uIJ0gWwmsB5YDwyrWuTLvj3XA+RXzTwXW5NeuK/qz\n",
       "HeB+OZvuXkOl3BfA7wMPAo+RfgUfXeJ98SlSEK4h9aY6pCz7gtQ6fgb4DelY/gdr+dmB1wCLgQ3A\n",
       "/cDx/dXkC8rMzErOD40xMys5B4GZWck5CMzMSs5BYGZWcg4CM7OScxCYmZWcg8DMrOQcBGZmJff/\n",
       "AcOVeMHQa0f8AAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106196410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b0 = np.array([[2],[2],[0]])\n",
    "a = 0.0000001\n",
    "\n",
    "def cost_func(b):\n",
    "    e = A.dot(b) - y\n",
    "    return e.T.dot(e)[0][0]\n",
    "\n",
    "def derivative_J(b, j):\n",
    "    Aj = A[:,j].reshape((len(A),1))\n",
    "    return 2*Aj.T.dot(A.dot(b) - y)[0]\n",
    "\n",
    "def gradient_J(b):\n",
    "    grad = [derivative_J(b, j) for j in range(len(b))]\n",
    "    return np.array(grad)\n",
    "\n",
    "\n",
    "N_itr = 10000\n",
    "J = []\n",
    "for i in range(N_itr):\n",
    "    b0 = b0 - a*gradient_J(b0)\n",
    "    J.append(cost_func(b0))\n",
    "\n",
    "print 'beta:\\n', b0\n",
    "\n",
    "e = A.dot(b0) - y #error vector\n",
    "print \"\\nRSS: \", e.T.dot(e)[0][0]\n",
    "\n",
    "plt.plot(range(N_itr), J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, plot the cost function vs number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent method discussed above is called _batch gradient descent_ because it iterates through all the data before calculating $\\bf \\beta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha \\sum_{i=1}^{n} (\\beta^T {\\bf x}_i - y_i) x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger data sets, this can take very long to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can compute the gradient for each iteration on the basis of single randomly picked examle ${\\bf x}_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha (\\beta^T {\\bf x}_i - y_i) x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general stochastic gradient descent converges to minimum much faster than the batch gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient method for the previous example. Since, we have very few points, we'll note pick example randomly. But still update $\\beta$ for each data point ${\\bf x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:\n",
      "[ 2.01508468  2.41143777  0.02156052]\n",
      "\n",
      "RSS:  203.098303633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1089aaad0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEACAYAAACkvpHUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAF2BJREFUeJzt3X+wZ3V93/HnCxc3tG6gBKvI8lPXH0SjK4EdjcgmGMWY\n",
       "ACYo2A7DRFozs7YmsY3RjApjm2k1VgpNYCbhNx0WHPHHNiJVqKsY3axG1FVAdkGa/cEPuyLExhaQ\n",
       "d/84n5v79Xbv3T137/d+v/fe52Pmzj3fz/n1OWe/+33dzzmfz/mmqpAkaV8dMOoKSJIWFoNDktSL\n",
       "wSFJ6sXgkCT1YnBIknoxOCRJvcwYHEmuTPJQki0DZS9N8pUk30qyIcmKVn5Mkh8nuaP9XDqwzglJ\n",
       "tiTZmuTigfLlSW5s5ZuSHD2Mg5QkzZ29tTiuAk6bUnY58K6q+gXgE8AfDMzbVlWr28+6gfLLgPOr\n",
       "ahWwKsnENs8Hdrfyi4APzvZAJEnzY8bgqKrbgUemFK9q5QC3Ar810zaSHA6sqKrNreha4Mw2fTpw\n",
       "TZu+CTh1H+stSRqR2dzj+E6SM9r0m4AjB+Yd2y5TbUzyqlZ2BLBjYJmdrWxi3naAqnoSeDTJobOo\n",
       "kyRpnswmON4KrEvyNeAZwOOtfBdwZFWtBt4JXD9x/0OStHgs67tCVX0XeB1AkucDb2jlj9NCpKq+\n",
       "nuReYBVdC2PlwCZWMtkC2QkcBexKsgw4uKp+MHWfSXygliTNQlVlrrfZOziSPLOqvp/kAOC9dDe+\n",
       "SXIY8EhV/STJcXShcV9V/TDJY0nWAJuBc4FL2uY2AOcBm4CzgNum2+8wDn4hSnJhVV046nqMA8/F\n",
       "JM/FJM/FpGH90T1jcCRZD5wCHJZkO3AB8Iwkb2+L3FRVV7fpVwMfSPIE8BTwO1X1wzZvHXA1cBBw\n",
       "c1Xd0sqvAK5LshXYDZwzJ0clSRqaGYOjqt4yzaxLphZU1ceBj0+znb8BXrKH8v8LvHnv1ZQkjQtH\n",
       "ji88G0ddgTGycdQVGCMbR12BMbJx1BVY7LIQvsgpSXmPQ5L6GdZnpy0OSVIvBockqReDQ5LUi8Eh\n",
       "SerF4JAk9WJwSJJ6MTgkSb0YHJKkXgwOSVIvBockqReDQ5LUi8EhSerF4JAk9WJwSJJ6MTgkSb0Y\n",
       "HJKkXgwOSVIvBockqReDQ5LUi8EhSerF4JAk9WJwSJJ6MTgkSb0YHJKkXgwOSVIvBockqReDQ5LU\n",
       "i8EhSerF4JAk9WJwSJJ6MTgkSb0YHJKkXgwOSVIvBockqZcZgyPJlUkeSrJloOylSb6S5FtJNiRZ\n",
       "MTDvPUm2Jrk7yWsHyk9IsqXNu3igfHmSG1v5piRHz/UBSpLm1t5aHFcBp00puxx4V1X9AvAJ4A8A\n",
       "khwPnA0c39a5NEnaOpcB51fVKmBVkoltng/sbuUXAR/cz+ORJA3ZjMFRVbcDj0wpXtXKAW4FfqtN\n",
       "nwGsr6onqup+YBuwJsnhwIqq2tyWuxY4s02fDlzTpm8CTp3tgUiS5sds7nF8J8kZbfpNwJFt+jnA\n",
       "joHldgBH7KF8Zyun/d4OUFVPAo8mOXQWdZIkzZNls1jnrcAlSd4HbAAen9sq7VmSCwdebqyqjfOx\n",
       "X0laKJKsBdYOez+9g6Oqvgu8DiDJ84E3tFk7mWx9AKyka2nsbNNTyyfWOQrYlWQZcHBV/WCa/V7Y\n",
       "t66StJS0P6g3TrxOcsEw9tP7UlWSZ7bfBwDvpbvxDV3r45wkT09yLLAK2FxVDwKPJVnTbpafC3xq\n",
       "YJ3z2vRZwG2zPhJJ0ryYscWRZD1wCnBYku3ABcAzkry9LXJTVV0NUFV3JvkocCfwJLCuqqottw64\n",
       "GjgIuLmqbmnlVwDXJdkK7AbOmasDkyQNRyY/28dXkqqq7H1JSdKEYX12OnJcktSLwSFJ6sXgkCT1\n",
       "YnBIknoxOCRJvRgckqReDA5JUi8GhySpF4NDktSLwSFJ6sXgkCT1YnBIknoxOCRJvRgckqReDA5J\n",
       "Ui8GhySpF4NDktSLwSFJ6sXgkCT1YnBIknoxOCRJvRgckqReDA5JUi8GhySpF4NDktSLwSFJ6sXg\n",
       "kCT1YnBIknoxOCRJvRgckqReDA5JUi8GhySpF4NDktSLwSFJ6sXgkCT1MmNwJLkyyUNJtgyUnZRk\n",
       "c5I7knw1yYmt/JgkP27ldyS5dGCdE5JsSbI1ycUD5cuT3NjKNyU5ehgHKUmaO3trcVwFnDal7EPA\n",
       "+6pqNfD+9nrCtqpa3X7WDZRfBpxfVauAVUkmtnk+sLuVXwR8cLYHIkmaHzMGR1XdDjwypfgB4OA2\n",
       "fQiwc6ZtJDkcWFFVm1vRtcCZbfp04Jo2fRNw6r5VW5I0Kstmsc67gS8l+TBd8LxiYN6xSe4AHgXe\n",
       "W1VfAo4Adgwss7OV0X5vB6iqJ5M8muTQqvrBLOolSZoHs7k5fgXwjqo6Cvh94MpWvgs4sl3Ceidw\n",
       "fZIVc1NNSdK4mE2L46Sqek2b/hhwOUBVPQ483qa/nuReYBVdC2PlwPormWyB7ASOAnYlWQYcPF1r\n",
       "I8mFAy83VtXGWdRdkhatJGuBtcPez2yCY1uSU6rqC8CvAPcAJDkMeKSqfpLkOLrQuK+qfpjksSRr\n",
       "gM3AucAlbVsbgPOATcBZwG3T7bSqLpxFXSVpyWh/UG+ceJ3kgmHsZ8bgSLIeOAU4LMl2ul5UbwP+\n",
       "LMly4MftNcCrgQ8keQJ4Cvidqvphm7cOuBo4CLi5qm5p5VcA1yXZCuwGzpmrA5MkDUeqatR12Ksk\n",
       "VVUZdT0kaSEZ1menI8clSb0YHJKkXgwOSVIvBockqReDQ5LUi8EhSerF4JAk9WJwSJJ6MTgkSb0Y\n",
       "HJKkXgwOSVIvBockqReDQ5LUi8EhSerF4JAk9WJwSJJ6MTgkSb0YHJKkXgwOSVIvBockqReDQ5LU\n",
       "i8EhSerF4JAk9WJwSJJ6MTgkSb0YHJKkXgwOSVIvBockqReDQ5LUi8EhSerF4JAk9WJwSJJ6MTgk\n",
       "Sb0YHJKkXgwOSVIvBockqZcZgyPJlUkeSrJloOykJJuT3JHkq0lOHJj3niRbk9yd5LUD5Sck2dLm\n",
       "XTxQvjzJja18U5Kj5/oAJUlza28tjquA06aUfQh4X1WtBt7fXpPkeOBs4Pi2zqVJ0ta5DDi/qlYB\n",
       "q5JMbPN8YHcrvwj44H4ejyRpyGYMjqq6HXhkSvEDwMFt+hBgZ5s+A1hfVU9U1f3ANmBNksOBFVW1\n",
       "uS13LXBmmz4duKZN3wScOsvjkCTNk2WzWOfdwJeSfJgueF7Ryp8DbBpYbgdwBPBEm56ws5XTfm8H\n",
       "qKonkzya5NCq+sEs6iVJmgezuTl+BfCOqjoK+H3gyrmtkiRpnM2mxXFSVb2mTX8MuLxN7wSOHFhu\n",
       "JV1LY2ebnlo+sc5RwK4ky4CDp2ttJLlw4OXGqto4i7pL0qKVZC2wdtj7mU1wbEtySlV9AfgV4J5W\n",
       "vgG4PslH6C5BrQI2V1UleSzJGmAzcC5wycA659Fd4joLuG26nVbVhbOoqyQtGe0P6o0Tr5NcMIz9\n",
       "zBgcSdYDpwCHJdlO14vqbcCfJVkO/Li9pqruTPJR4E7gSWBdVVXb1DrgauAg4OaquqWVXwFcl2Qr\n",
       "sBs4Z/q6kCpquvmSpPmRyc/28ZWkmMyMb9IF2C1VPD66WknSeEtSVZW9L9lzuwswOPbk28AFwM1V\n",
       "/J/5qZUkjTeDo/9Vqu3AHwGfqOJ/z32tJGm8GRxzc3vjKeD3gPVV/K+52KAkjSuDY7j3xf8DcHkV\n",
       "9w1zJ5I0nwyO+e9QdSPwX4BNVfxkvncuSfvL4BiPnrj3Ah+gu2/yd6OujCTNxOAYj+CYzofpxqR8\n",
       "17EmksaFwbHwPo+/CvwJ8Okq/n7UlZG09BgcCy84pnMx3fO9vmPrRNIwGRyL+zN2B/DHwEer8JHy\n",
       "kuaEwbG4g2M6nwH+FLjVx6tI6svgWJrBMZ2/oLsZ/9Uqnhp1ZSSNJ4PD4NgXf0L31bzeP5FkcBgc\n",
       "++XfAddjd2FpSTE4/Lwbhj8G1gN3GijS4mNw+Lk2nz5C10L5ho9bkRYug8PgGAfXAtcBX/J7T6Tx\n",
       "Z3AYHOPsi3Q9vT5Txe5RV0ZSx+AwOBaiH9H19Po43X0Uuw5L88jgMDgWm4/TXfa6zScNS8NhcBgc\n",
       "S8Xf092cvwn4lq0UafYMDoND8BXgauAvq9g14rpIY8/gMDg0s5voxqTcWsWjo66MNA4MDoNDs3cN\n",
       "8FHgi1X8aNSVkeaLwWFwaDiupGutGCpadAwOg0Pz7wbgY8Dn/Z4ULUQGh8Gh8fI5ustfnwP+1md9\n",
       "aRwZHP6/1MLxPbr7Kp+he96XX8KlkTA4DA4tHjcAn6C7r/LgqCujxcvgMDi0NGynG1F/C/C1Kn48\n",
       "4vpoATM4DA4J4K/obtjfRvf8Lx97r2kZHAaHtC/+O/BJ4PPAVh/ZsrQZHAaHNBc+C2wA/gdwjy2W\n",
       "xc3gMDik+fDXdDfub6N7yKQ9whYwg8PgkMbBA8CNdONXNgGPOIZlfBkcvjelheLTwM3A7cDdVTwx\n",
       "4vosWSMJjiRXAm8AHq6ql7SyG4AXtEUOAX5YVauTHAPcBdzd5n2lqta1dU6gexz2zwA3V9XvtvLl\n",
       "dN9j/XJgN3B2Vf3PPdTD4JAWj0foRt3fSndpbIetluEYVXCcTPf1n9dOBMeU+R+mC45/34Ljv02z\n",
       "3GbgX1XV5iQ3A5dU1S1J1gEvrqp1Sc4G3lhV5+xhfYNDWlq20fUO2wh8DXjYcOlvWMFxwEwzq+p2\n",
       "ur8O9lShAG+m+w6EaSU5HFhRVZtb0bXAmW36dLpHM0D3hNJT963akha55wH/FvhL4EHgqYSa8nNP\n",
       "wocTfiPh8IQ5/4DUns0YHHtxMvBQVd07UHZskjuSbEzyqlZ2BLBjYJmdrWxi3naAqnoSeDTJoftR\n",
       "J0lLxyrg39B1L97FnsPl+wl/kfDPEp6X7Ndnnppl+7HuW4DrB17vAo6sqkeSvBz4ZJKf36/a/ZQD\n",
       "PgDHHQYvfh687bnwa8+du21LWqQOA/5F+wEge26XfIZubMuX6bohL8jvZkmyFlg77P3MKjiSLAPe\n",
       "SHdTG4Cqehy6Pt9V9fUk99L9RbATWDmw+komWyA7gaOAXW2bB1fVHr/3oOqpC6avDwGOBH6V7vLZ\n",
       "a2dzXJKWrNe3H2DacHmQ7tLZF4HNwLZxG0BZVRvp7gsBkGTaz839Mdtm22uAu6pq10RBksOSPK1N\n",
       "H0cXGvdV1QPAY0nWtPsi5wKfaqttAM5r02fRDTrqrYqq4m+ruKKK11WRwR+63l9vAP58NtuXJODZ\n",
       "dC2Xa+l6jz65h0tjlfCdhIsSzko4djFeHttbr6r1wCnAzwEPA++vqquSXEXX3fbPB5b9TeADwBPA\n",
       "U23ZT7d5E91xD6LrjvuOVr6c7kmgq+m6455TVffvoR5D6RnQbZunAS8CXkd3+e2EYexHkqa4i24g\n",
       "5ZeBO4D7qnhyLnew5AcADis49r5vfhZ4JfDrwNuAA0dRD0lL1sN0D6/8K+Bv6AZV7tM9GINjRMEx\n",
       "k3Zv5Tl0N6POpLvcJkmj8iXgC8BXgS2Qew2OBaZd2zyObnzK6cCvjbZGkpaWYHAsMi1Yngv8MvAb\n",
       "dJfDJGmOGByLLjj2ZqCb8cl0XQX/+WhrJGlhMTiWXHDsi4RDgRPpuki/CTh6tDWSND4MDoNjFhKW\n",
       "AS8EXk3XavFymLRkGBwGx5AkHEw3fuUUunstq0dbI0lzw+AwOEak3Wv5p8BJdF2Pz6C7qS9prBkc\n",
       "BscYa+HybOAX6XqJ/TrdY2ckjYzBYXAscC1cDqF7OOar6R7zsmaklZIWNYPD4FgiEpbT3dB/BV3r\n",
       "5TfZv68AkJYog8Pg0E9JeCZd6+UVdI/Uf+VoaySNG4PD4FBvbXT+UXQB80q68S4vHWmlpHljcBgc\n",
       "GpqBgHkZk5fIThxppaT9ZnAYHBq5ga7JL6brnvxq4LSRVkqalsFhcGhBaTf5V9ENqJxoxbxwpJXS\n",
       "EmNwGBxalFor5mfpvolyNd29mF8GjhhlvbQYGBwGh8Q/PCLmhXQhs4buctlxI62UxpTBYXBIPbXL\n",
       "Zc8DXkL3PLJXAL800kppHhkcBoc0RO2S2WHAC+iC5kS6kHn+KOul/WFwGBzSGBl4PtkL6XqZTYyV\n",
       "MWjGhsFhcEgLWAuaf0J36ex4ujEzJ9FdPtNQGBwGh7TEtC8iW0nXinkxXYeAk7BVs48MDoND0l4l\n",
       "HAgcSTeG5ufpHjHzi3StnCXG4DA4JA1Fe+TMM+m+oOwFdK2bicfPHDTCqu0ng8PgkDRWWutmJXAs\n",
       "k5fTXkrXG20MPrMMjjH4R5Ck/ZfwNOBZwDF0nQVeRNcF+uXA4XO4J4Nj1PWQpFFL+BkmWzkTofNi\n",
       "uvs4K6YsbXCMuh6StJAM67PzgLneoCRpcTM4JEm9GBySpF4MDklSLwaHJKkXg0OS1MuMwZHkyiQP\n",
       "JdkyUHZDkjvaz/eS3DEw7z1Jtia5O8lrB8pPSLKlzbt4oHx5khtb+aYkR8/1AUqS5tbeWhxXAacN\n",
       "FlTVOVW1uqpWAze1H5IcD5xN9yCx04BLk0z0H74MOL+qVgGrkkxs83xgdyu/CPjgHBzTopZk7ajr\n",
       "MC48F5M8F5M8F8M3Y3BU1e3AI3ua10LhzcD6VnQGsL6qnqiq+4FtwJokhwMrqmpzW+5a4Mw2fTpw\n",
       "TZu+CTh1lsexlKwddQXGyNpRV2CMrB11BcbI2lFXYLHbn3scJwMPVdW97fVzgB0D83cAR+yhfGcr\n",
       "p/3eDlBVTwKPJjl0P+okSRqy/QmOtwDXz1VFJEkLw7LZrJRkGfBGuic5TthJ9+UpE1bStTR2tump\n",
       "5RPrHAXsats8uKp+MM0+x/+hWvMkyQWjrsO48FxM8lxM8lwM16yCA3gNcFdV7Roo2wBcn+QjdJeg\n",
       "VgGbq6qSPJZkDbAZOBe4ZGCd84BNwFnAbXvamQ84lKTxsbfuuOuBLwPPT7I9yW+3WWczeVMcgKq6\n",
       "E/gocCfwGWBdTT56dx1wObAV2FZVt7TyK4CfS7IV+D3g3ft/SJKkYVoQj1WXJI2PsR85nuS0NqBw\n",
       "a5I/HHV9hiHJ/Um+1QZVbm5lhyb5XJJ7knw2ySEDy/caaDnOphlkOmfHvpAGmU5zLi5MsmNg0O3r\n",
       "B+Yt5nNxZJLPJ/lOkm8neUcrX3LvjRnOxejeG1U1tj/A0+jGgxwDHAh8A3jRqOs1hOP8HnDolLIP\n",
       "Ae9q038I/Mc2fXw7Dwe287KNyZbjZuCkNn0zcNqoj20fjv1kYDWwZRjHTneZ9NI2fTZww6iPuee5\n",
       "uAB45x6WXezn4tnAy9r0M4Dv0n3T3ZJ7b8xwLkb23hj3FsdJdPdE7q+qJ4Ab6AYaLkZTOwAMDo68\n",
       "hslBk7MZaDm2as+DTOfy2BfMINNpzgX8/+8NWPzn4sGq+kab/hFwF12nmyX33pjhXMCI3hvjHhz/\n",
       "MECwmRhUuNgUcGuSryX5l63sWVX1UJt+iO6L7WF2Ay0Xmrk89sUwyPRfJ/lmkisGLs0smXOR5Bi6\n",
       "lthfs8TfGwPnYlMrGsl7Y9yDY6ncuf+l6p799Xrg7UlOHpxZXftxqZyLn7KUj725DDgWeBnwAPCf\n",
       "Rlud+ZXkGXR/Af9uVf3d4Lyl9t5o5+JjdOfiR4zwvTHuwTF1UOGR/HRiLgpV9UD7/X3gE3SX6B5K\n",
       "8myA1sR8uC3eZ6DlzuHWfGjm4tinDjKdGLg67SDTcVRVD1dD16X9pDZr0Z+LJAfShcZ1VfXJVrwk\n",
       "3xsD5+K/TpyLUb43xj04vkb3NN1jkjyd7qbNhhHXaU4l+UdJVrTpfwy8FtjC5OBI2u+J/zgbgHOS\n",
       "PD3JsUwOtHwQeCzJmiShG2j5SRamuTj2T+1hW9MOMh1X7cNxwhvp3huwyM9Fq/sVwJ1V9Z8HZi25\n",
       "98Z052Kk741R9xjYhx4Fr6frRbANeM+o6zOE4zuWrgfEN4BvTxwjcChwK3AP8FngkIF1/qidj7uB\n",
       "1w2Un9DePNuAS0Z9bPt4/OuBXcDjdNdYf3sujx1YTjcwdSvddeFjRn3MPc7FW+luYH4L+Cbdh+Sz\n",
       "lsi5eBXwVPt/cUf7OW0pvjemORevH+V7wwGAkqRexv1SlSRpzBgckqReDA5JUi8GhySpF4NDktSL\n",
       "wSFJ6sXgkCT1YnBIknr5f8jQp0mUSSCuAAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1065f1d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#implement SGD\n",
    "b0 = np.array([2.,2.,0.])\n",
    "a = 0.000001\n",
    "\n",
    "def cost_func(b):\n",
    "    e = A.dot(b) - y\n",
    "    return e.T.dot(e)[0][0]\n",
    "\n",
    "J = []\n",
    "N_itr = 1000\n",
    "for n in range(N_itr):\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(b0)):\n",
    "            b0[j] = b0[j] - a * (b0.dot(A[i]) - y[i][0])*A[i][j]\n",
    "        J.append(cost_func(b0))\n",
    "        \n",
    "        \n",
    "print 'beta:\\n', b0\n",
    "\n",
    "e = A.dot(b0.reshape(len(b0),1)) - y #error vector\n",
    "print \"\\nRSS: \", e.T.dot(e)[0][0]\n",
    "\n",
    "\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##General Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In several machine learning algrithms, we wish to learn a hypothesis function, $h({\\bf x})$ given the data set $\\{ {\\bf x}_i\\}$. For linear regression, the hypothesis function is $h = {\\bf \\beta}^T {\\bf x}$. The parameters $\\bf \\beta$ are _learned_ by optimizing the cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J({\\bf \\beta}) = \\sum_{i=1}^{n} (h({\\bf x}_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find $\\beta$ by optimizing the cost function, using batch gradient descent method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha \\sum_{i=1}^{n} \\nabla_j \\left( h ({\\bf x}_i) - y_i \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\nabla_j$ represents the derivative w.r.t to $\\beta_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger data sets we can use stochastic gradient method instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha  \\nabla_j \\left( h ({\\bf x}_i) - y_i \\right)^2$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
